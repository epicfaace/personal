<!-- 
      Formatted using
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
  -->
<html>
  <head>
    <style type="text/css">
      mytitle { font-size: 300%; text-align: center; display: block }
      author { font-size: 200%; text-align: center; display: block }
      version { font-size: 200%; text-align: center; display: block }
      h1 { font-size: x-large }
      body { font-size: large }
      comment { display:none }
      footnote { display:none }
      biblio { display:none }
      bibref { font-weight: bold }
      bibid { font-weight: bold }
      term { }
      term::before { content: open-quote }
      term::after { content: close-quote }
    </style>
  </head>
  <body>
    <mytitle>Parsing: a timeline</mytitle>
    <version>Version 3.0</version>
    <author>Jeffrey Kegler</author>
    <h1>4th BCE: Pannini's description of Sanskrit</h1>
    <p>In India, Pannini creates an exact and complete description of
      the Sanskrit language, including pronunciation. Sanskrit could be
      recreated using nothing but Pannini's grammar.
      Pannini's grammar is
      probably the first formal system of any kind, predating Euclid. Even
      today, nothing like it exists for any other natural language of
      comparable size or corpus. Pannini is the object of serious study
      today. But in the 1940's and 1950's Pannini is almost unknown in
      the West. It will have no direct effect on the other events in
      this timeline.
    </p>
    <h1>1906: Markov's chains</h1>
    <p>Andrey Markov introduces his
      <term>chains</term>
      -- a set of
      states with transitions between them.
      One offshoot of Markov's work will be what we now
      know as regular expressions.
      Markov uses his chains, not
      for parsing, but for solving problems in probability<a id="footnote-1-ref" href="#footnote-1">[1]</a>.
    </p>
    <h1>1943: Post's rewriting system</h1>
    <p>Emil Post defines and studies a formal rewriting system
      <a id="footnote-2-ref" href="#footnote-2">[2]</a>
      using productions.
      With this, the process of rediscovering Pannini in the
      West begins.</p>
    <h1>1945: Turing discovers stacks</h1>
    <p>Alan Turing discovers the stack as part of his design of the
      ACE machine. This is important in parsing because recursive parsing
      requires stacks. The importance of Turing's discovery is not noticed
      at the time and stacks will be re-discovered many times over the
      next two decades<a id="footnote-3-ref" href="#footnote-3">[3]</a>.
    </p>
    <h1>1948: Shannon repurposes Markov's chains</h1>
    <p>Claude Shannon publishes the foundation paper of information theory<a id="footnote-4-ref" href="#footnote-4">[4]</a>.
      In this
      paper, Shannon makes an attempt to model English using Andrey Markov's
      chains.<a id="footnote-5-ref" href="#footnote-5">[5]</a>
    </p>
    <h1>1949: Rutishauser's compiler</h1>
    <p>From 1949 to 1951 at the ETH Zurich, Heinz Rutishauser worked on
      the design of what we would now call a compiler<a id="footnote-6-ref" href="#footnote-6">[6]</a>.
      Rutishauser's language is line-by-line and parsed as hoc, but it
      does parse arithmetic expressions. Rutishauser's expression parser
      did not honor precedence but did allow nested parentheses. It
      is perhaps the first algorithm which can really be considered a
      parsing method. Rutishauser's compiler was never implemented.</p>
    <h1>Operator Expressions</h1>
    <p>With Rutishauser's work, the parsing of operator expressions comes
      into the picture.
      Operators expressions are
      often treated a side issue.
      In fact, as this timeline will show,
      operator expressions are in fact central to the story of Western parsing theory.
    </p>
    <p>
      Informally, operator are expressions built up from operands
      and operators.
      An operand can be another operator expression,
      so that operators expressions can be built up recursively.
    </p>
    <p>
      The archetypal examples of operator expressions are arithmetic expressions:
      "2+(3*4)", "13**2**-5*9/11", "1729-42*8675309", etc.
      In Western mathematics these were read according traditional ideas of
      associativity and precedence.
      Traditionally, in the above
    </p><ul>
      <li>'**' is exponentiation.
        It right associates and has tightest<a id="footnote-7-ref" href="#footnote-7">[7]</a>
        precedence.
      </li>
      <li>
        Multiplication ('*') and division ('/') left associate.
        They have a precedence equal to each other
        and less tight than exponentiation.
      </li>
      <li>
        Addition ('+') and subtraction ('-') left associate.
        They have a precedence equal to each other
        and less tight than multiplication and division.
      </li>
      <li>
        Parentheses, when present, override the traditional
        associativity and precedence.
      </li>
    </ul>
    <p>Certainly in the first attempts at parsing in compilers,
      arithmetic expressions are central.
      The first compiled languages are organized line-by-line and,
      except for arithmetic expressions,
      lines are
      interpreted using basic string manipulations.
      It is only when those string manipulations turn to dealing
      with arithmetic expressions that they become sophisticated
      enough to be called parsing techniques,
      and having a parsing theory to describe them becomes helpful.
    </p>
    <h1>1950: Boehm's compiler</h1>
    <p>During 1950, Corrado Boehm, also at the ETH Zurich develops his
      own compiler. They are working at the same institution at the same
      time, but Boehm is unaware of Rutishauser's work until his own is
      complete. Like Rutishauser, Boehm's language is line-by-line and
      parsed ad hoc, except for expressions. Boehm's expression parser
      <em>does</em>
      honor precedence, making it perhaps the first operator precedence
      parser<a id="footnote-8-ref" href="#footnote-8">[8]</a>.
      Boehm's compiler also allows parentheses, but the two cannot
      be mixed -- an expression can either be parsed using precedence
      or have parentheses, but not both.</p>
    <p>Boehm's is also the first self-compiling compiler -- it is written
      in its own language. Like Rutishauser's, Boehm's compiler was never
      implemented<a id="footnote-9-ref" href="#footnote-9">[9]</a>.
    </p>
    <h1>1952: Grace Hopper uses the term &quot;compiler&quot;</h1>
    <p>Grace Hopper writes a linker-loader.
      <a
      href="https://en.wikipedia.org/wiki/History_of_compiler_construction#First_compilers%22">She
        calls it a &quot;compiler&quot;</a>. Hopper seems to be the first
      person to use this term for a computer program.</p>
    <h1>Term: &quot;Compiler&quot; as of 1952</h1>
    <p>Hopper used the term &quot;compiler&quot; in a meaning it
      had at the time: &quot;to compose out of materials from other
      documents&quot;<a id="footnote-10-ref" href="#footnote-10">[10]</a>.
      Specifically, before Hopper, the task we now see as
      &quot;compiling&quot; was then seen putting together a set of
      pre-existing assembler subroutines and calling them. Hopper's new
      program went one step further -- instead of calling the subroutines
      it expanded them (or in other words &quot;compiled&quot; them) into
      a single program. Since Hopper the term has acquired a different
      and very specialized meaning in the computer field. Today we would
      not call Hopper's program a &quot;compiler&quot;<a id="footnote-11-ref" href="#footnote-11">[11]</a>.
    </p><p>There were compilers written or described before Hopper,
      but nobody called them compilers -- compiling in our modern sense
      was called
      "automatic coding", "codification automatique" or "Rechenplanfertigung"<a id="footnote-12-ref" href="#footnote-12">[12]</a>.
    </p><h1>1952: Glennie's AUTOCODE</h1>
    <p>Knuth
      Knuth and Pardo 1976,
      p 42.
      calls Glennie's the first
      <q>real</q><a id="footnote-13-ref" href="#footnote-13">[13]</a>
      compiler, by which he means that
      it was actually implemented and used by someone to translate
      algebraic statements into machine language.
      Glennie's AUTOCODE
      was very close to the machine -- too close to implement operator expressions.
      AUTOCODE was hard-to-use, and had little impact on
      other users of its target -- the Manchester Mark I.
      And because
      Glennie worked for the British atomic weapons projects his papers
      were routinely classified, so that the influence of AUTOCODE was
      slow to spread. Nonetheless, many other
      <term>compilers</term>
      afterward were named AUTOCODE, and this probably indicates some
      awareness of Glennie's effort<a id="footnote-14-ref" href="#footnote-14">[14]</a>.
    </p>
    <h1>1954: The FORTRAN project begins</h1>
    <p>At IBM, a team under John Backus begins working on the language which will be called FORTRAN.</p>
    <h1>&quot;Compiler&quot; as of 1954</h1>
    <p>As of 1954, the term &quot;compiler&quot; was still being used
      in Hopper's looser sense, instead of its modern, specialized,
      one. In particular, there was no implication that the output of a
      &quot;compiler&quot; is ready for execution by a computer.<a id="footnote-15-ref" href="#footnote-15">[15]</a>
      The output of one 1954 &quot;compiler&quot;, for example, produced
      relative addresses, which needed to be translated by hand before
      a machine can execute them.</p>
    <h1>1955: Noam Chomsky starts teaching at MIT</h1>
    <p>Noam Chomsky is awarded a Ph.D. in linguistics and accepts a
      teaching post at MIT. MIT does not have a linguistics department and
      Chomsky is free to teach his own approach,
      Chomksy's linguistics course is
      highly original and very mathematical.</p>
    <h1>1955: Work begins on the IT compiler</h1>
    <p>At Purdue, a team including Alan Perlis and Joseph Smith begins work
      on the IT compiler<a id="footnote-16-ref" href="#footnote-16">[16]</a>.
    </p>
    <h1>1956: The IT compiler is released</h1>
    <p>Perlis and Smith, now at the Carnegie Institute of Technology,
      finish the IT compiler. Don Knuth calls this
    </p><blockquote>
      the first really
      <em>useful</em>
      compiler. IT and IT's derivatives were used successfully and
      frequently in hundreds of computer installations until [its
      target,] the [IBM] 650 became obsolete. [... P]previous systems
      were important steps along the way, but none of them had the
      combination of powerful language and adequate implementation and
      documentation needed to make a significant impact in the use of
      machines.<a id="footnote-17-ref" href="#footnote-17">[17]</a>.
    </blockquote>
    <p>The IT language had arithmetic expressions, of a sort --
      parentheses are honored, but otherwise evaluation is always
      right-to-left -- there is no operator precedence. IT did honor
      parentheses, but nonetheless its way of doing arithmetic expressions
      proves very unpopular: Donald Knuth reports that &quot;The lack of
      operator priority (often called precedence or hierarchy) in the IT
      language was the most frequent single cause of errors by the users
      of that compiler.&quot;<a id="footnote-18-ref" href="#footnote-18">[18]</a>
    </p>
    <h1>&quot;Compiler&quot; as of 1956</h1>
    <p>In the 1956 document describing the IT compiler<a id="footnote-19-ref" href="#footnote-19">[19]</a>,
      IT team is careful to define the term. Their definition makes clear that
      they are using of the word
      <term>compiler</term>
      in something like its
      modern sense, perhaps for the first time. From this time on, when used
      as a technical term within computing,
      <term>compiler</term>
      will usually
      mean what we currently understand it to mean.</p>
    <h1>1956: The Chomsky hierarchy</h1>
    <p>Chomsky publishes the paper which is usually considered the foundation of Western formal language theory.<a id="footnote-20-ref" href="#footnote-20">[20]</a>
      Chomsky demolishes the idea that natural language grammar can be
      modeled using only Markov chains. Instead, the paper advocates a
      natural language approach that uses three layers:</p>
    <ul>
      <li>Chomsky uses
        Markov's chains
        as his
        <b>bottom layer</b>.
        This becomes the modern compiler's
        <b>lexical phase</b>.</li>
      <li>Chomsky's
        <b>middle layer</b>
        uses context-free
        grammars and context-sensitive grammars.
        These are his own
        discoveries<a id="footnote-21-ref" href="#footnote-21">[21]</a>.
        This middle layer becomes the
        <b>syntactic phase</b>
        of modern
        compilers.</li>
      <li>Chomsky's
        <b>top layer</b>, again his own
        discovery, maps or
        <term>transforms</term>
        the output of the middle
        layer. Chomsky's top layer is the inspiration for AST transformation
        phase of modern compilers.</li>
    </ul>
    <h1>Term: &quot;Parsing&quot;</h1>
    <p>Chomsky is a turning point, so much so that it establishes or settles the
      meaning of many of the terms we are using. &quot;Parsing&quot;,
      for our purposes, is transforming a string of symbols into a
      structure. Typically this structure is a parse tree.</p>
    <h1>Recognizers</h1>
    <p>In contrast to a parser,
      a
      <term>recognizer</term>
      is a program which takes a string and a grammar and answers
      <q>yes</q>
      or
      <q>no</q>
      --
      <q>yes</q>
      if the string is in the language described by the grammar,
      <q>no</q>
      otherwise.
      For a compiler to work, it clearly needs
      a parser, not a recognizer.
    </p><p>In comparing algorithms, it can be important to keep in mind
      whether they are recognizers or parsers.
      Recognizers can be,
      to some extent, pressed into service as parsers by introducing captures.
      This is often done, for example, with regular expressions,
      and if the semantics is very simple, it works.
    </p>
    <h1>1957: Kleene's regular expressions</h1>
    <p>Steven Kleene discovers regular expressions, a very handy notation
      for Markov chains. It will turn out that several other mathematical objects
      being studied at this time
      are also equivalent to regular expressions: the various
      finite state automata; and some of the objects being studied as
      neural nets.</p>
    <h1>1957: Chomsky publishes
      <cite>Syntactic Structures</cite></h1>
    <p>Noam Chomsky publishes
      <em>Syntactic Structures</em>, one of the most important books of
      all time. The orthodoxy in 1957 is structural linguistics which
      argues, with Sherlock Holmes, that
      <q>it is a capital mistake
        to theorize in advance of the facts</q>. Structuralists start
      with the utterances in a language, and build upward.</p>
    <p>But Chomsky claims that without a theory there are no facts: there
      is only noise. The Chomskyan approach is to start with a grammar,
      and use the corpus of the language to check its accuracy. Chomsky's
      approach will soon come to dominate linguistics.</p>
    <h1>Term: &quot;Chomskyan parsing&quot;</h1>
    <p>In computing, parsing theory mainly follows Chomsky's work in
      linguistics. Parsing is &quot;Chomksyan&quot; if it is guided by
      a BNF grammar. From this point on, most parsers and most parsing
      theory will be Chomskyan; and this timeline will focus on Chomskyan
      parsing. But, as we shall see, non-Chomskyan parsing does survive
      and has its users today.</p>
    <h1>1957: FORTRAN released</h1>
    <p>Backus's team makes the first FORTRAN compiler available to IBM
      customers. FORTRAN is the first high-level language that will find
      widespread implementation. As of this writing, it is the oldest
      language that survives in practical use.</p>
    <p>FORTRAN is a line-by-line language and its parsing is pre-Chomskyan
      and ad hoc. But it includes one important discovery. FORTRAN I was
      line-by-line, but it allowed expressions. And, learning from the
      dissatisfaction with the compiler, FORTRAN honors associativity
      and precedence.</p>
    <p>The designers of FORTRAN discovered a strange trick --
      they hacked the expressions by adding parentheses around each
      operator. Surprisingly, this works. In fact, once the theoretical
      understanding of operator precedence comes about, the FORTRAN
      I implementation is actually a hackish and inefficient way of
      implementing precedence.</p>
    <h1>1958: LISP released</h1>
    <p>John McCarthy's LISP appears. LISP goes beyond the line-by-line
      syntax -- it is recursively structured. But the LISP interpreter does
      not find the recursive structure: the programmer must explicitly
      indicate the structure herself, using parentheses.
      Similarly, LISP does not have operator expressions in the usual sense --
      associativity and precedence must be specified with parentheses.
    </p>
    <h1>1959: Backus's notation</h1>
    <p>Backus discovers a new notation to describe the IAL language.
      (aka ALGOL)<a id="footnote-22-ref" href="#footnote-22">[22]</a>
    </p>
    <h1>1960: BNF</h1>
    <p>Peter Naur improves the Backus notation and uses it to describe
      ALGOL 60. The improved notation will become known as Backus-Naur Form
      (BNF).</p>
    <h1>1960: The ALGOL report</h1>
    <p>The ALGOL 60 report specifies, for the first time, a block
      structured language. ALGOL 60 is recursively structured but the
      structure is implicit -- newlines are not semantically significant,
      and parentheses indicate syntax only in a few specific cases. The
      ALGOL compiler will have to find the structure. It is a case of 1960's
      optimism at its best. As the ALGOL committee is well aware, a parsing
      algorithm capable of handling ALGOL 60 does not yet exist. But the
      risk they are taking will soon pay off.</p>
    <p>With the ALGOL 60 report, a quest begins which continues to this day: the search for a parser that is</p>
    <ul>
      <li>efficient,</li>
      <li>practical,</li>
      <li>declarative, and</li>
      <li>general.</li>
    </ul><p>
      On one hand, the
      risk they are taking will pay off immediately, producting vastly improved
      parsers and parsing theory.
      On the other hand, the parser they seek will not be discovered for
      three decades,
      two more decades will pass before it is
      implemented.
    </p>
    <h1>Term:
      <term>declarative</term></h1>
    <p>For our purposes, a parser is
      <term>declarative</term>,
      if it
      will parse directly and automatically from grammars written in BNF.
      Declarative parsers are often called
      <term>syntax-driven</term>
      parsers.
    </p>
    <h1>Term:
      <term>procedural</term></h1>
    <p>A parser is
      <term>procedural</term>, if it requires procedural
      logic as part of its syntax phase.
      If a parser is procedural, then it is not declarative.
    </p><h1>Term:
      <term>general</term></h1>
    <p>A general parser is a parser that will parse
      any grammar that can be written in BNF<a id="footnote-23-ref" href="#footnote-23">[23]</a>.
      This is a very useful property
      -- it makes it easy for a grammar-writer to know that her grammar
      will parse.
      This in turn
      makes it easy to auto-generate grammars,
      because it is easy for a program to know that
      the grammar it generates will successfully parse.
      This opens the way
      to true second-order languages -- languages which specify other
      languages.</p>
    <h1>1960: Gleenie's compiler-compiler</h1>
    <p>A.E. Gleenie publishes his description of a compiler-compiler<a id="footnote-24-ref" href="#footnote-24">[24]</a>.
      Glennie's
      <term>universal compiler</term>
      is more of a methodology
      than an implementation -- the compilers must be written by
      hand.
    </p><p>
      Glennie have been the first to use BNF as
      a description of a
      <em>procedure</em>
      instead of as the description of a
      <em>Chomsky grammar</em><a id="footnote-25-ref" href="#footnote-25">[25]</a>
      Glennie points out that the distinction is
      <q>important</q><a id="footnote-26-ref" href="#footnote-26">[26]</a>.
    </p>
    <p>BNF and Chomsky's context free grammars were created primarily as means
      of grammar
      <b>description</b>.
      It is for this reason that BNF is often used in standards and language
      manuals, where it is expected that the readers of the BNF will be people
      not machines.
      Given a BNF notation,
      a human can very reasonably determine exactly what strings are and
      are not in a language, and the structure of the parses of those
      strings.
    </p>
    <p>
      Of course, a notation exact enough to determine what string are and
      are not in a language,
      and their parses,
      and which does so in a way accessible to human readers,
      could drive a computer algorithm.
      At least so you would hope and expect.
    </p>
    <p>
      As this timeline will show, a practical declarative
      parser proved elusive.
      BNF remained an attractive notation, and in the place of
      declarative parsers, there arose, beginning with Glennie,
      parsers which are driven by BNF notation, but which do
      not parse the language described by that notation.
    </p>
    <p>
      BNF
      notation, when used to describe a procedure, is a set of instructions,
      to be tried in some order, and used to process a string. Procedural
      BNF describes a procedure first, and a language only indirectly.</p>
    <p>
      I will call these
      <term>pseudo-declarative</term>
      parsers.
      To be sure, Glennie in
      <a href="#bib-Glennie_1960">Glennie 1960</a>never pretends this parser is declarative.
      Glennie points the distinction out carefully,
      and notes that it is
      <q>important</q><a id="footnote-27-ref" href="#footnote-27">[27]</a>.
      But many subsequent writers will be less clear on the matter,
      and many more of the readers will suffer because of it.
      Many will waste time assuming that they can get eventually their test
      suite to write by writing BNF which describes it.
      Others will get their test suite to work,
      and will think (falsely)
      that this means that the language their parser parses is that
      specified by the BNF.
    </p><p>Both procedural and Chomskyan BNF describe languages, but usually
      <em>not the same</em>
      language. This is an important point, and one which will be
      overlooked many times in the years to come.</p>
    <p>[TODO: needs revision. ]
      The pre-Chomskyan approach, using procedural BNF, is far more
      natural to someone trained as a computer programmer. The parsing
      problem appears to the programmer in the form of strings to be
      parsed, exactly the starting point of procedural BNF and pre-Chomsky
      parsing.</p>
    <p>Even when the Chomskyan approach is pointed out, it does not
      at first seem very attractive. With the pre-Chomskyan approach,
      the examples of the language more or less naturally lead to a
      parser. In the Chomskyan approach the programmer has to search for
      an algorithm to parse strings according to his grammar -- and the
      search for good algorithms to parse Chomskyan grammars has proved
      surprisingly long and difficult. Handling semantics is more natural
      with a Chomksyan approach. But, using captures, semantics can be
      added to a pre-Chomskyan parser and, with practice, this seems
      natural enough.</p>
    <p>Despite the naturalness of the pre-Chomskyan approach to parsing,
      we will find that the first fully-described automated parsers
      are Chomskyan. This is a testimony to Chomsky's influence at the
      time. We will also see that Chomskyan parsers have been dominant
      ever since.</p>
    <h1>1960: Operator precedence and stacks</h1>
    <p>Since FORTRAN I, many people have refined its operator precedence implementation.
      A Feb 1960 paper by Samuelson and Bauer<a id="footnote-28-ref" href="#footnote-28">[28]</a>
      describes in detail the use of stacks to implement
      operator precedence.
      This paper was very influential.
    </p>
    <h1>1961: The first parsing paper</h1>
    <p>In January, Ned Irons publishes a paper describing his ALGOL
      60 parser. It is the first paper to fully describe any parser. The
      Irons algorithm is Chomskyan and top-down with a bottom-up &quot;left
      corner&quot; element -- it is what now would be called a &quot;left
      corner&quot; parser.<a id="footnote-29-ref" href="#footnote-29">[29]</a>
    </p>
    <p>You would expect parsing might start with hard-written
      parsers for specific grammar classes.
      But in fact, this first parser is declarative and general.
      And since it is general,
      operator expressions are well within the power of
      <a href="#bib-Irons_1961">Irons 1961</a>.
    </p>
    <h1>Terms:
      <term>Top-down</term></h1>
    <p>A top-down parser deduces the consituents of a rule from the rule.
      That is, it looks at the rule first,
      and then deduces what is on its RHS.
      Thus, it works from start rule, and works
      <q>top down</q>
      until it arrives at the input tokens.
    </p>
    <p>It is important to note that no useful parser can be purely top-down --
      if parser worked purely pure top-down, it would never look at its input.
      So every top-parser we will consider has *some* kind of bottom-up element.
      That bottom-up element may be very simple -- for example, one character lookahead.
      The
      <a href="#bib-Irons_1961">Irons 1961</a>
      parser,
      like most modern top-down parsers,
      has a sophisticated bottom-up element.
    </p>
    <h1>Terms:
      <term>Bottom-up</term></h1>
    <p>A bottom-up parser deduces a rule from its constituents.
      That is, it looks at either the input or the LHS symbols
      of previously deduced rules,
      and from that deduces a rule.
      Thus, it works from the input tokens, and works
      <q>bottom up</q>
      until it reaches the start rule.
    </p>
    <p>Many programmers have a fixation on classifying parsers in terms
      of top-down and bottom-up.
      As we saw, the
      <a href="#bib-Irons_1961">Irons 1961</a>
      parser is not simply top-down or
      bottom-up, though arguably describing it in terms of top-down and
      bottom-up components is helpful. But for other parsing algorithms,
      top-down vs. bottom-up classification is a pointless pedantry --
      the classification can be done, but tells you nothing about the
      actual behavior of the parser.</p>
    <h1>Terms: &quot;Synthetic attribute&quot;</h1>
    <p><a href="#bib-Irons_1961">Irons 1961</a>
      also introduces synthetic attributes: the parse creates
      a tree, which is evaluated bottom-up. Each node is evaluated using
      attributes &quot;synthesized&quot; from its child nodes.<a id="footnote-30-ref" href="#footnote-30">[30]</a>
    </p>
    <p>Synthetic attributes are a concept in semantics, not parsing,
      but they will be important for us.
      Almost everyone doing practical parsing does so because
      she intends to move on to a semantics phase, and
      feedback from new
      semantic concepts often has major effects on the development of
      parsing.</p>
    <h1>1961: Lucas discovers recursive descent</h1>
    <p>Peter Lucas publishes the first description of a purely top-down parser<a id="footnote-31-ref" href="#footnote-31">[31]</a>.
      Either Irons paper or this one can be considered to be recursive descent<a id="footnote-32-ref" href="#footnote-32">[32]</a>.
      Certainly the Lucas algorithm more closely resembles modern implementations of recursive descent.</p>
    <p>Except to say that he deals properly with them, Lucas does not say
      how he parses operator expressions. But it is easy to believe Lucas'
      claim -- by this time there are several specialized techniques for
      parsing operator expressions and Lucas cites
      <a href="#bib-Samuelson_and_Bauer_1960">Samuelson and Bauer 1960</a>.
    </p>
    <p>For our purposes, it is sufficient to note that while recursive
      descent cannot parse arithmetic expressions directly, it can deal
      with them in two ways:</p>
    <ul>
      <li><p>Recursive descent can switch to another algorithm when
          it encounters an operator expression. This is easy to do in a
          hand-written implementation.</p></li>
      <li><p>It can parse the operands and operands into a list, and
          the list can be reparsed in post-processing. This is essentially
          a delayed-action form of the first method.</p></li>
    </ul>
    <p[ TODO -- Rewrite. ]
    Lucas' writeup most likely describes a hand-written
      parser, not a declarative one.
      In the 1960s,
      Hand-coded approaches became more popular than declarative
      approaches like that of Irons due to three factors:</p>
      <ul>
        <li><p>Memory and CPU were both extremely limited. Hand-coding
            paid off, even when the gains were small.</p></li>
        <li><p>Top-down parsing is intuitive -- it essentially means
            calling subroutines. It therefore requires little or no knowledge of
            parsing theory. This makes it a good fit for hand-coding.</p></li>
        <li><p>Pure recursive descent is, in fact, a very weak parsing
            technique. Pure recursive descent cannot
            deal with either of the operator grammars we have described:
            <a href="g-basic">basic</a>
            or
            <a href="g-right-recursive">right-recursive</a>.
            Recursive descent has severe limits,
            it is often necessary to go beyond them,
            and that helps justify
            coding the entire parser by hand.</p>
        </li>
      </ul>
      <h1>1961: Dijkstra's shunting yard algorithm</h1>
      <p>In November 1961, Dijkstra publishes the &quot;shunting yard&quot; algorithm.<a id="footnote-33-ref" href="#footnote-33">[33]</a>
        <a
href="https://www.engr.mun.ca/~theo/Misc/exp_parsing.htm">Theodore
          Norvell has made an attempt at classification</a>
        of the botanical profusion of operator expression parsers. All
        earlier parsers are subspecies of what he calls &quot;the classic
        algorithm&quot;, so that Dijkstra's approach is new. The number
        of level of precedence is not &quot;built in&quot; into the
        shunting-yard algorithm. And the shunting-yard algorithm is more
        efficient than the classic approaches: Code size does not grow,
        and the algorithm's speed does not decrease, with the number of
        precedence levels. Dijkstra's algorithm is not a game-changer --
        it has no major impact on the Operator Issue -- but it offers a
        genuinely different implementation alternative.</p>
      <h1>The operator issue as of 1961</h1>
      <p>Originally the operator issue was that it required a parsing
        algorithm at a time
        when there were no parsing algorithms.
        With Lucas paper, the operator issue takes a new form --
        one which it still has over 50 years later.
        Specifically, recursive descent cannot parse this very simple operator grammar:
      </p id="g-basic"><ul>
        <li>S ::= E</li>
        <li>E ::= E + T</li>
        <li>E ::= T</li>
        <li>T ::= T * F</li>
        <li>T ::= F</li>
        <li>F ::= number</li>
      </ul><p>
        In fact recursive descent cannot parse any left recursive grammar.
      </p>
      <p>
        But the problem is even more severe than that.
        For human readers, left association is natural for most operators,
        and they would find reversing it to be very confusing.
        But suppose we ignore that face and rewrite the above grammar to be right
        recursive:
      </p id="g-right_recursive"><ul>
        <li>S ::= E</li>
        <li>E ::= T + E</li>
        <li>E ::= T</li>
        <li>T ::= F * T</li>
        <li>T ::= F</li>
        <li>F ::= number</li>
      </ul><p>
        Recursive descent cannot parse this grammar either --
        the lookahead required is just too much for it.
      </p><p>This means that by itself, recursive descent is never
        sufficient for practical grammars -- it must always be part of a
        hybrid.
      </p>
      <h1>1963: Non-Chomskyan approaches</h1>
      <p>L. Schmidt, Howard Metcalf, and Val Schorre present papers on
        compilers at a Denver conference<a id="footnote-34-ref" href="#footnote-34">[34]</a>.
        This approach is
        non-Chomskyan: BNF is used as a
        procedural notation, and does not necessarily describe the
        syntax parsed.
        They call their approach syntax-driven.
        In fact,
        their approach is pseudo-declarative
        and therefore,
        according to the terminology used in this timeline,
        not truly syntax-driven.
      </p><h1>1964: The Meta II compiler</h1>
      <p>Schorre publishes a paper on the Meta II
        <q>compiler
          writing language</q>, summarizing the papers of the 1963
        conference. Schorre cites both Backus and Chomsky as sources for
        Meta II's notation. Schorre notes that his parser is
        <q>entirely different</q>
        from that of
        <a href="#bib-Irons_1961">Irons 1961</a>
        -- as noted, it is
        pre-Chomskyan.
        [ TODO: relate non-Chomskyan to declarative. ]
        Meta II is a template, rather than something that
        readers can use, but in principle it can be turned into a fully
        automated compiler-compiler<a id="footnote-35-ref" href="#footnote-35">[35]</a>.
      </p>
      <h1>1965: Knuth discovers LR</h1>
      <p>Don Knuth discovers LR parsing<a id="footnote-36-ref" href="#footnote-36">[36]</a>.
          </footnote></footnote></p>
      <h1>Term: "linear time"</h1>
      <p>[ TODO.  And where to put this?
      </p>
      <h1>Misconception: deterministic parsing versus deterministic parsers</h1>
      <p>Knuth in his 1965 investigates the question:
        "What is the best a deterministic parser can do?".
        Many people mistook this as answering a similar-sounding but
        different question:
        "What is the best way to parse a deterministic grammar?".
      </p>
      <p>More precisely, Knuth found the largest class of grammars that
        a deterministic parser could parse in linear time,
        and found an algorithm to do it.
        (It's called LR(k).)
        Many people took that to be the answer to the question:
        "Which deterministic grammars can be parsed in linear time,
        and what is the best algorithm for that."
        It seems reasonable to guess the two questions are essentially
        the same,
        and have the same answer.
        In fact, most people as the time, and since,
        have taken that guess as a fact.
        But as we will see,
        in 1991 that guess turns out to be wrong.
      </p><h1>1968: Lewis and Stearns discover LL</h1>
      <p>When Knuth discovered the LR grammars, he announced them to
        the world with a full-blown mathematical description. The top-down
        grammars, which arose historically, lack such a description. In 1968,
        Lewis and Stearns fill that gap by defining the LL(k) grammars<a id="footnote-37-ref" href="#footnote-37">[37]</a>.
      </p>
      <h1>Terms:
        <term>LL</term>,
        <term>LR</term>,
        <term>RL</term>
        and
        <term>RR</term></h1>
      <p>When LL is added to the vocabulary of parsing, the meaning of
        <term>LR</term>
        shifts slightly. In 1965 Knuth meant LR to mean
        <q>translatable from left to right</q><a id="footnote-38-ref" href="#footnote-38">[38]</a>.
        LL means
        <q>scan from the left, using left reductions</q>
        and LR acquires its current meaning of
        <q>scan from the left, using
          right reductions</q><a id="footnote-39-ref" href="#footnote-39">[39]</a>.
      </p><p>If there is a number after the parentheses in this notation for
        parsing algorithms, it usually indicates the number of tokens of
        lookahead.
        As an example, LL(1) --
        <q>scan from the
          left, using left reductions with one character of lookahead</q>
        --
        will be important in what follows.
      </p>
      <h1>LL(1) and the Operator Issue</h1>
      <p>
        With
        <a href="#bib-Knuth_1965">Knuth 1965</a>
        and
        <a href="#bib-Lewis_and_Stearns_1968">Lewis and Stearns 1968</a>,
        a precise terminology falls into place for describing
        the "Operator Issue".
        [ TODO: Go back to Operator Issue terminology? ]
        We can state what the problem has been with recursive descent.
        Recursive Descent, in its pure form, is LL(1).
        Arithmetic operator grammars are not LL(1) -- no even close.
        In fact neither
        the
        <a href="#g-basic">Basic Operator Grammar</a>
        or its
        <a href="#right-recursive">right recursive variant</a>
        is LL(k) for any k.
      </p>
      <p>
        Something has to give as long as you
        are using recursive descent. And this is a major reason why truly
        declarative versions of LL(1) are not used. A pure declarative
        LL(1) parser generator
        <em>could</em>
        be written, but it would not be able to parse arithmetic expressions
        properly.</p>
      <h1>1968: Earley's algorithm</h1>
      <p>Jay Earley discovers the algorithm named after him<a id="footnote-40-ref" href="#footnote-40">[40]</a>.
        Like the Irons
        algorithm, Earley's algorithm is Chomskyan, declarative and fully
        general. Unlike the Irons algorithm, it does not backtrack. Earley's
        algorithm is both top-down and bottom-up at once -- it uses dynamic
        programming and keeps track of the parse in tables. Earley's approach
        makes a lot of sense and looks very promising indeed, but there are
        three serious issues:</p>
      <ul>
        <li>First, there is a bug in the handling of zero-length rules.</li>
        <li>Second, it is quadratic for right recursions.</li>
        <li>Third, the bookkeeping required to set up the tables is,
          by the standards of 1968 hardware, daunting.</li>
      </ul>
      <h1>1968: Attribute grammars</h1>
      <p>Knuth publishes a paper on a concept he had been working for the
        previous few years: attribute grammars<a id="footnote-41-ref" href="#footnote-41">[41]</a>
        Irons' synthetic attributes had always been inadequate for many
        problems, and had been supplemented by side effects or state
        variables. Knuth adds inherited attributes, and discovers attribute
        grammars.</p>
      <h1>Term:
        <term>Inherited attributes</term></h1>
      <p>Recall that a node in parse gets its synthetic attributes from
        its parents. Inherited attributes are attibutes a node gets from
        its parents. Of course, this creates potential circularities, but
        inherited attributes are powerful and, with care, the circularities
        can be dealt with.</p>
      <h1>Term:
        <term>Attribute grammar</term></h1>
      <p>An attribute grammar is a grammar whose node may have both inherited and synthetic attributes.</p>
      <h1>1969: LALR</h1>
      <p>Frank DeRemer describes a new variant of Knuth's LR
        parsing. DeRemer's LALR algorithm requires only a stack and a state
        table of quite manageable size. LALR looks practical.</p>
      <h1>1969: the
        <tt>ed</tt>
        editor</h1>
      <p>Ken Thompson writes the
        <tt>ed</tt>
        editor as one of the
        first components of UNIX. At this point, regular expressions are an
        esoteric mathematical formalism. Through the
        <tt>ed</tt>
        editor
        and its descendants, regular expressions will become an everyday
        part of the working programmer's toolkit.</p>
      <h1>1972: Aho and Ullman is published</h1>
      <p>Alfred Aho and Jeffrey Ullman publish the first volume
        <a id="footnote-42-ref" href="#footnote-42">[42]</a>
        of
        their two volume textbook
        summarizing the theory of parsing.
        This book is still important. It
        is also distressingly up-to-date -- progress in parsing theory
        slowed dramatically after 1972. Aho and Ullman's version
        of Earley's algorithm includes
        a straightforward fix to the zero-length rule bug in Earley's
        original<a id="footnote-43-ref" href="#footnote-43">[43]</a>.
        Unfortunately, this fix involves adding even
        more bookkeeping to Earley's.</p>
      <p>Under the names TDPL and GTDPL, Aho and Ullman investigate
        the non-Chomksyan parsers in the Schorre lineage<a id="footnote-44-ref" href="#footnote-44">[44]</a>. They note that
        <q>it can be quite difficult to determine what language is
          defined by a TDPL parser<q><a id="footnote-45-ref" href="#footnote-45">[45]</a>.
            That is, GTDPL parsers do whatever
            they do, and that whatever is something the programmer in general
            will not be able to describe. The best a programmer can usually
            do is to create a test suite and fiddle with the GTDPL description
            until it passes. Correctness cannot be established in any stronger
            sense. GTDPL is an extreme form of the old joke that
            <q>the code is
              the documentation</q>
            -- with GTDPL nothing documents the language
            of the parser, not even the code.</q></q></p>
      <p>GTDPL's obscurity buys nothing in the way of additional parsing
        power. Like all non-Chomskyan parsers, GTDPL is basically a extremely
        powerful recognizer. Pressed into service as a parser, it is
        comparatively weak. As a parser, GTDPL is essentially equivalent to
        Lucas's 1961 algorithm, which was in turn a restricted
        form of recursive descent.</p>
      <p>At or around this time, rumor has it that the main line of
        development for GTDPL parsers is classified secret by the US
        government<a id="footnote-46-ref" href="#footnote-46">[46]</a>.
        GTDPL parsers have the property that even small changes in
        GTDPL parsers can be very labor-intensive. For some government
        contractors, GTDPL parsing provides steady work for years to
        come. Public interest in GTDPL fades.</p>
      <h1>1973: Pratt parsing</h1>
      <p>One approach to the Operator Issue has not been mentioned. We
        have noted that LL(1) cannot parse operator expressions. What about
        making the entire grammar an operator grammar? That way you don't
        have to mix algorithms.</p>
      <p>It's an idea that had already occurred to
        Samuelson and Bauer in 1960.
        They intended their operator parser as a parser for ALGOL.
      </p><p>There are many problems with switching to operator
        grammars. Operator grammar are non-Chomskyan -- the BNF no longer
        accurately describes the grammar<a id="footnote-47-ref" href="#footnote-47">[47]</a>. Instead the BNF becomes part of
        a combined notation, and the actual grammar parsed depends also on
        precedence and semantics.
        And operator grammars have a very restricted
        form -- most practical languages are not operator grammars.</p>
      <p>But many practical grammars are almost operator grammars. And the
        Chomskyan approach has always had its dissenters.
        Vaughn Pratt was one
        of these, and discovered a new approach (the third and last in
        <a
    href="https://www.engr.mun.ca/~theo/Misc/exp_parsing.htm">Theodore
          Norvell's taxonomy</a>) to operator expression parsing, one which
        some have adopted as an overall solution to their parsing problems<a id="footnote-48-ref" href="#footnote-48">[48]</a>.
      </p>
      <p>The Pratt approach, and its descendant, precedence climbing,
        is not popular as an overall strategy.
        It is most often used as an alternative to the classic
        and the shunting yard approached,
        within a recursive descent strategy.
        All
        operator expression subparsers break the Chomskyan paradigm so the
        non-Chomskyan nature of Pratt's parser is not a problem in this
        context. But with respect to the Operator Issue, like Dijkstra's
        alternative, it is an implementation choice, not a game-changer.</p>
      <h1>1975: The C compiler is converted to LALR</h1>
      <p>Bell Labs converts its C compiler from hand-written recursive
        descent to DeRemer's LALR algorithm<a id="footnote-49-ref" href="#footnote-49">[49]</a>.</p>
      <h1>1977: The first dragon book is published</h1>
      <p>The first "dragon book"<a id="footnote-50-ref" href="#footnote-50">[50]</a>
        comes out. This soon-to-become
        classic textbook is nicknamed after the drawing on the front cover,
        in which a knight takes on a dragon. Emblazoned on the knight's lance
        are the letters
        <q>LALR</q>. From here on out, to speak lightly
        of LALR will be to besmirch the escutcheon of parsing theory.</p>
      <h1>1979: Yacc is released</h1>
      <p>Bell Laboratories releases Version 7 UNIX<a id="footnote-51-ref" href="#footnote-51">[51]</a>. V7 includes what is,
        by far, the most comprehensive, useable and easily available compiler
        writing toolkit yet developed.</p>
      <p>Part of the V7 toolkit is Yet Another Compiler Compiler
        (YACC)<a id="footnote-52-ref" href="#footnote-52">[52]</a>. YACC is LALR-powered. Despite its name, YACC is the first
        compiler-compiler in the modern sense. For some useful languages, the
        process of going from Chomskyan specification to executable is fully
        automated. Most practical languages, including the C language and
        YACC's own input language, still require manual hackery. Nonetheless,
        after two decades of research, it seems that the parsing problem
        is solved.</p>
      <h1>1987: Perl 1 is released</h1>
      <p>Larry Wall introduces Perl 1<a id="footnote-53-ref" href="#footnote-53">[53]</a>. Perl embraces complexity like no
        previous language. Larry uses YACC and LALR very aggressively --
        to my knowledge more aggressively than anyone before or since.</p>
      <h1>1990: Monads</h1>
      <p>
        Wadler starts to introduce monads to the functional programming community.
        As one example he converts his previous efforts at function parsing
        techniques<a id="footnote-54-ref" href="#footnote-54">[54]</a>.
        The parsing technique is pure recursive descent.
        The examples shown for monadic parsing are very simple,
        and do not include operator expressions.
        In his earlier non-monadic work, Wadler uses a very restricted form of operator expression:
        His grammar avoided left recursion by using parentheses,
        and operator precedence was not implemented<a id="footnote-55-ref" href="#footnote-55">[55]</a>.
      </p>
      <h1>1991: Leo's speed-up of Earley's algorithm</h1>
      <p>Joop Leo discovers a way of speeding up right recursions in
        Earley's algorithm<a id="footnote-56-ref" href="#footnote-56">[56]</a>. Leo's algorithm is linear for just about every
        unambiguous grammar of practical interest, and many ambiguous ones
        as well. In 1991 hardware is six orders of magnitude faster than
        1968 hardware, so that the issue of bookkeeping overhead had receded
        in importance. This is a major discovery. When it comes to speed,
        the game has changed in favor of the Earley algorithm.</p>
      <p>But Earley parsing is almost forgotten. Twenty years will
        pass before anyone writes a practical implementation of Leo's
        algorithm.</p>
      <h1>The 1990's</h1>
      <p>Earley's is forgotten. So everyone in LALR-land is content,
        right? Wrong. Far from it, in fact. Users of LALR are making
        unpleasant discoveries. While LALR automatically generates their
        parsers, debugging them is so hard they could just as easily write the
        parser by hand. Once debugged, their LALR parsers are fast for correct
        inputs. But almost all they tell the users about incorrect inputs
        is that they are incorrect. In Larry's words, LALR is
        <q>fast
          but stupid</q><a id="footnote-57-ref" href="#footnote-57">[57]</a>.
      </p>
      <h1>1992: Combinator parsing</h1>
      <p>Combinators are parsers which can be "combined" to compose other parsers.
        This allows you to have an algebra of parsers.
        It's an extremely attractive idea,
        and suggestive of great power,
        but in fact underneath it is nothing but recursive descent.
      </p>
      <p>
        Combinator parsing
        was introduced in two 1992 papers<a id="footnote-58-ref" href="#footnote-58">[58]</a>.
        Of more interest
        to us is the one by Hutton, which focuses on combinator parsing<a id="footnote-59-ref" href="#footnote-59">[59]</a>.
        Combinators will become important in programming languages and
        semantics. But in terms of parsing algorithms, Hutton introduces
        nothing new -- wrapped up in an attribute grammar; underlying the
        exciting new mathematics is the decades-old recursive descent,
        unchanged.
      </p><p>Hutton's example language is very basic --
        his expressions require left association,
        but not precedence.
        Left association is implemented by post-processing.
      </p>
      <p>
        Hutton's main presentation does not use monads.
        In his final pages, however,
        Hutton points out
        "combinator parsers give rise to a monad" and shows
        how his presentation could be rewritten in a form
        closely related to monads<a id="footnote-60-ref" href="#footnote-60">[60]</a>.
      </p>
      <h1>1995: Wadler's monads</h1><p>
        Today monads are a hot topic.
        In their adaptation into the world of
        programming,
        the 1995 article by Wadler
        <a id="footnote-61-ref" href="#footnote-61">[61]</a>
        is a turning point.
        One of Wadler's examples is a parser based on
        monads and combinator parsing.
        From here on out,
        an combinator parsing will become the standard
        example of monad programming.
      </p>
      <p>In its monadic form,
        the already attractive technique of combinator parsing is extremely elegant
        and certainly seems as if it *should* solve all parsing problems.
        But, once again, it is still recursive descent.
        Wadler handles left recursion by rewriting into
        right recursive list, and re-processing the list
        into left recursive form.
        Wadler's example grammar only has one operator,
        so the issue of precedence does not arise.
      </p>
      <p>
        Instead of one paradigm to solve them all,
        Wadler ends up with a two layered approach,
        with a hackish bottom layer and an awkward interface.
        This undercuts
        the simplicity and elegance of the combinator approach.
      </p>
      <h1>1996: Hutton and Meijer on cominator parsing</h1><p>
        Wadler had used parsing as an example to motivate
        his presentation of monads.
        In 1996, Graham Hutton and Erik Meijer<a id="footnote-62-ref" href="#footnote-62">[62]</a>
        take the opposite perspective -- they
        write a paper on combinator parsing that could
        "also be viewed as a first introduction to the use
        of monads in programming."
      </p>
      <h1>1999: Haskell
        <cite>Gentle Introduction</cite></h1><p>
        TODO:
        <a id="footnote-63-ref" href="#footnote-63">[63]</a>
      </p><h1>2000: The Perl 6 effort begins</h1>
      <p>Larry Wall decides on a radical reimplementation of Perl --
        Perl 6. Larry does not even consider using LALR again.</p>
      <h1>2002: Aycock and Horspool solve Earley's zero-length rule problem</h1>
      <p>John Aycock and R. Nigel Horspool publish their attempt at a
        fast, practical Earley's parser<a id="footnote-64-ref" href="#footnote-64">[64]</a>. Missing from it is Joop Leo's
        improvement -- they seem not to be aware of it. Their own speedup
        is limited in what it achieves and the complications it introduces
        can be counter-productive at evaluation time. But buried in their
        paper is a solution to the zero-length rule bug. And this time the
        solution requires no additional bookkeeping.</p>
      <h1>2004: Bryan Ford publishes his paper on PEG</h1>
      <p>Ford<a id="footnote-65-ref" href="#footnote-65">[65]</a>fills this gap by
        repackaging the nearly-forgotten GTDPL. Ford adds packratting,
        so that PEG is always linear, and provides PEG with an attractive
        new syntax.</p>
      <p>Implementers by now are avoiding YACC, but the demand for
        declarative parsers remains. PEG is not, in fact, declarative,
        but it uses the same BNF notation, and many users don't know the
        difference. And nothing has been done to change
        <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2015/03/peg.html">the problematic behaviors</a>
        of GTDPL.</p>
      <h1>2006: GNU C reverts to recursive descent</h1>
      <p>
        <q>The old Bison-based C and Objective-C parser has been replaced by a new, faster hand-written recursive-descent
          parser.<q><a id="footnote-66-ref" href="#footnote-66">[66]</a>
            Bison, an LALR parser, is the direct descendant of Steve Johnson's YACC.
          </q></q></p>
      <p>
        With this single line in middle of a change list hundreds of lines long.
        GNU announces a major event in parsing history.
        For three decades, the industry's flagship C compilers have
        used LALR as their parser -- proof of the claim that LALR and serious
        parsing are equivalent.
        Now, GNU replaces LALR with the technology
        that it replaced a quarter century earlier: recursive descent.</p>
      <h1>2010: Leo 1991 is implemented</h1><p>
        TODO:
        <a id="footnote-67-ref" href="#footnote-67">[67]</a>
      </p><h1>2012: General precedence parsing</h1><p>
        TODO:
        <a id="footnote-68-ref" href="#footnote-68">[68]</a>
      </p><h1>Today</h1>
      <p>After five decades of parsing theory, the state of the art seems
        to be back where it started. We can imagine someone taking Ned Iron's
        original 1961 algorithm from the first paper ever published describing
        a parser, and republishing it today. True, he would have to translate
        its code from the mix of assembler and ALGOL into something more
        fashionable, say Haskell. But with that change, it might look like
        a breath of fresh air.
      </p>
      <h1>Marpa: an afterword</h1>
      <p>The recollections of my teachers cover most of this timeline. My
        own begin around 1970. Very early on, as a graduate student, I became
        unhappy with the way the field was developing. Earley's algorithm
        looked interesting, and it was something I returned to on and off.</p>
      <p>Recall that the original vision of the 1960's was a parser that
        was efficient, practical, general, and declarative.</p>
      <p>By 2010 this vision seemed to have gone the same way as many other
        1960's dreams. The rhetoric stayed upbeat, but parsing practice had
        become a series of increasingly desperate compromises.</p>
      <p>But, while nobody was looking for them, the solutions to
        the problems encountered in the 1960's had appeared in the
        literature. Aycock and Horspool had solved the zero-length rule
        bug. Joop Leo had found the speedup for right recursion. And the
        issue of bookkeeping overhead had pretty much evaporated on its
        own. Machine operations are now a billion times faster than in 1968,
        and are probably no longer relevant in any case -- cache misses are
        now the bottleneck.</p>
      <p>The programmers of the 1960's would have been prepared to trust
        a fully declarative Chomskyan parser. With the experience with LALR
        in their collective consciousness, modern programmers might be more
        guarded. As Lincoln said,
        <q>Once a cat's been burned, he won't
          even sit on a cold stove.</q>
        But I found it straightforward to
        rearrange the Earley parse engine to allow efficient event-driven
        handovers between procedural and declarative logic. And Earley
        tables provide the procedural logic with full knowledge of the state
        of the parse so far, so that Earley's algorithm is a better platform
        for hand-written procedural logic than recursive descent.</p>
      <h1>Partial Bibliography</h1>
      <p>This bibliography is incomplete --
        it contains only documents
        repeatedly mentioned in this timeline.
        Bibliographic information for the
        less frequently mentioned
        sources is in the individual footnotes.</p>
      <p>
        <b id="bib-Aho_and_Ullman_1972">Aho and Ullman 1972</b>:
        Alfred V. Aho and Jeffrey D. Ullman,
        <cite>The Theory of Parsing, Translation and Compiling:
          Volume I: Parsing</cite>, Prentice Hall, Englewood Cliffs, N.J., 1972.
      </p>
      <p>
        <b id="bib-Aho_and_Ullman_1973">Aho and Ullman 1973</b>:
        Alfred V. Aho and Jeffrey D. Ullman,
        <cite>The Theory of Parsing, Translation and Compiling:
          Volume II: Compiling</cite>, Prentice Hall, Englewood Cliffs, N.J., 1973.
      </p><p>
        <b id="bib-Earley_1968">Earley 1968</b>:
        Jay Earley,
        <cite>An Efficient Context-free Parsing Algorithm</cite>,
        PhD thesis, Carnegie-Mellon Univ., Pittsburg, Pa., 1968.
      </p>
      <p>
        <b id="bib-Aho_and_Ullman_1977">Aho and Ullman 1977</b>:
        Alfred V. Aho and Jeffrey D. Ullman,
        <cite>Principles of compiler design</cite>,
        Addision-Wesley,
        Reading, Mass. (1977)
        The dragon on the cover is green,
        and this classic first edition
        is sometimes called the "green dragon book" to distinguish
        it from its second edition,
        which had a red dragon on the cover.
      </p>
      <p>
        <b id="bib-Aycock_and_Horspool_2002">Aycock and Horspool 2002</b>:
        John Aycock and R. Nigel Horspool, "Practical Earley parsing",
        <cite>Computer J.</cite>
        45(6):620–630. (2002)
      </p>
      <b id="bib-Ford_2002">Ford 2002</b>:
        Bryan Ford, "Packrat parsing: a practical linear-time algorithm with backtracking", Master’s thesis,
        Massachusetts Institute of Technology. (September 2002.)
        [http://pdos.csail.mit.edu/papers/packrat-parsing:ford-ms.pdf.]
      </p>
      <p>
        <b id="bib-Ford_2004">Ford 2004</b>:
        Bryan Ford,
        "Parsing expression grammars: A recognition-based syntactic foundation",
        in
        <cite>Proceedings
          of the 31st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages,
          POPL 2004 (Venice, Italy, 14–16 January 2004)</cite>, N. D. Jones and X. Leroy, Eds., ACM, pp. 111–122.
        (January 2004.)
      </p>
      <p>
        <b id="bib-Glennie_1960">Glennie 1960</b>:
        A. E. Glennie,
        "On the Syntax Machine and the Construction of a Universal Compiler"
        Computation Center, Carnegie Institute of Technology,
        <cite>Technical Report No. 2</cite>
        (10 July 1960).
        http://www.chilton-computing.org.uk/acl/literature/reports/p024.htm
      </p>
      <p>
        <b id="bib-Grune_and_Jacobs_2008">Grune and Jacobs 2008</b>:
        Grune, D., & Jacobs, C. J. H. (2008).
        <cite>Parsing Techniques: A Practical
          Guide, 2nd edition</cite>.
        (<cite>Monographs in Computer Science</cite>). New York, USA:
        Springer.
        When it comes to determining who first discovered an idea,
        the literature often disagrees.
        In such cases, I have often deferred
        to the judgement of Grune and Jacobs.
      </p>
      <p>
        <b id="bib-Hutton_1992">Hutton 1992</b>:
        Hutton, "Higher order functions for parsing",
        Journal of Functional Programming 2(3):323-343, July 1992.
      </p>
      <p>
        <b id="bib-Hutton_and_Meijer_1996">Hutton and Meijer 1996</b>:
        Graham Hutton and Erik Meijer.
        <cite>Technical Report NOTTCS-TR-96-4</cite>,
        Department of Computer Science, University of Nottingham, 1996.
      </p>
      <p><b id="bib-Irons_1961">Irons 1961</b>:
        Irons, E. T. A syntax-directed compiler for ALGOL 60. Commun. ACM,
        4(1):51–55, Jan. 1961.
      </p>
      <p>
        <b id="bib-Knuth_1965">Knuth 1965</b>:
        D. E. Knuth, "On the translation of languages from left to right"
        <cite>Information and Control</cite>, Vol. 8, Issue: 6, Page: 607-639 (1965).
      </p>
      <p>
        <b id="bib-Knuth_1968">Knuth 1968</b>:
        Donald E. Knuth, "Semantics of context-free languages"
        Math. Syst. Theory, 2(2):127– 145, (1968).
      </p>
      <p>
        <b id="bib-Knuth_and_Pardo_1976">Knuth and Pardo 1976</b>:
        D. E. Knuth and L. Trabb Pardo, "The Early Development of
        Programming Languages,"
        <cite>Report STAN-CS-76-562</cite>, Computer Science Dept.,
        Stanford Univ., Stanford, Calif. (Aug. 1976).
        [http://bitsavers.org/pdf/stanford/cs_techReports/STAN-CS-76-562_EarlyDevelPgmgLang_Aug76.pdf ].
      </p>
      <p>
        <b id="bib-Leo_1991">Leo 1991</b>:
        Joop M. I. M. Leo,
        "A general context-free parsing algorithm running in linear time on
        every LR(k) grammar without using lookahead",
        Theoret. Comput. Sci., 82:165–176, 1991.
      </p>
      <p>
        <b id="bib-Lewis_and_Stearns_1968">Lewis and Stearns 1968</b>:
        P. M. Lewis, II and R. E .Stearns, "Syntax-directed transduction",
        <cite>Journal of the ACM</cite>,
        15(3):465– 488 (1968).
      </p>
      <p>
        <b id="bib-Lucas_1961">Lucas 1961</b>:
        Lucas, P. Die Strukturanalyse von Formelnübersetzern / analysis of
        the structure of formula translators. Elektronische Rechenanlagen,
        3(11.4):159–167, 1961, (in German).
      </p>
      <p>
        <b id="bib-Pratt_1973">Pratt 1973</b>:
        Vaughan R. Pratt, "Top down operator precedence", in
        <cite>First
          ACM Symposium on Principles of Programming Languages<cite>,
            pages 41–51, ACM. (Oct. 1973.)
          </cite></cite></p>
      <p>
        <b id="bib-Samuelson_and_Bauer_1960">Samuelson and Bauer 1960</b>:
        Samelson, K. and Bauer, F. L. Sequential formula translation. Commun. ACM,
        3(2):76–83, Feb. 1960.
      </p>
      <p>
        <b id="bib-Snyder_1975">Snyder 1975</b>:
        Alan Snyder, "A Portable Compiler for the Language C",
        MIT, Project MAC, Cambridge MA. (May 1975.)
      </p>
      <p>
        <b id="bib-McIlroy_and_Kernighan_1979">McIlroy and Kernighan 1979</b>:
        M. D. McIlroy and B. W. Kernighan, editors,
        <cite>UNIX Programmer's Manual</cite>, Seventh Edition, Volume 1,
        Bell Labs, Murray Hill, New Jersey.
        (January 1979.)
      </p>
      <p><b id="bib-Wadler_1985">Wadler 1985</b>:
        Philip Wadler, "How to Replace Failure by a List of Successes: A method
        for exception handling, backtracking, and pattern matching in lazy
        functional languages". in J-P Jouannaud (ed.),
        <cite>Functional Programming
          Languages and Computer Architecture</cite>.
        <cite>Lecture Notes in Computer Science</cite>, 1985
        vol. 201, Springer-Verlag GmbH, pp. 113-128. DOI: 10.1007/3-540-15975-4_33.
      </p>
    </p[><h1>Footnotes</h1>
<p id="footnote-1">1.
        Andrey Andreyevich Markov, "Extension of the law of large numbers to quantities, depending on each other" (1906).
        The title is a translation -- the original in Russian.
 <a href="#footnote-1-ref">&#8617;</a></p>
<p id="footnote-2">2.
        Emil L. Post,
        "Formal Reductions of the General Combinatorial Decision Problem",
        <cite>American Journal of Mathematics</cite>,
        Vol. 65, No. 2, pp. 197-215 (April 1943).
        DOI: 10.2307/2371809
 <a href="#footnote-2-ref">&#8617;</a></p>
<p id="footnote-3">3.
        Carpenter, B. E.; Doran, R. W. (January 1977). "The other Turing machine". The Computer Journal. 20 (3): 269–279.
 <a href="#footnote-3-ref">&#8617;</a></p>
<p id="footnote-4">4.
        "A Mathematical Theory of Communication"
 <a href="#footnote-4-ref">&#8617;</a></p>
<p id="footnote-5">5.
pp. 4-6.
 <a href="#footnote-5-ref">&#8617;</a></p>
<p id="footnote-6">6.
        Knuth and Pardo 1976,
        pp. 29-35, 40.
 <a href="#footnote-6-ref">&#8617;</a></p>
<p id="footnote-7">7.
          This timeline refers to precedence levels as "tight"
          or "loose".
          The traditional terminology is confusing: tighter
          precedences are "higher", but traditionally the
          precendence levels are also numbered and the higher
          the number, the lower the precedence.
 <a href="#footnote-7-ref">&#8617;</a></p>
<p id="footnote-8">8.
        Many of the "firsts" for parsing algorithms in this
        timeline are debatable,
        and the history of operator precedence is especially murky.
        Here I follow
        <a href="#bib-Samuelson_and_Bauer_1960">Samuelson and Bauer 1960</a>
        in giving the priority to Boehm.
 <a href="#footnote-8-ref">&#8617;</a></p>
<p id="footnote-9">9.
        Knuth and Pardo 1976,
        pp 35-42.
 <a href="#footnote-9-ref">&#8617;</a></p>
<p id="footnote-10">10.
        Quoted definition is from Nora B. Moser,
        "Compiler method of automatic programming",
        Symposium on Automatic Programming for Digital Computer,
        ONR, p. 15.,
        as cited in
        Knuth and Pardo 1976,
        p 51.
 <a href="#footnote-10-ref">&#8617;</a></p>
<p id="footnote-11">11.
        I hope nobody will interpret this terminological clarification as in any sense
        detracting from Hopper's achievement.
        Whatever it is called, Hopper's program was a major advance,
        both in terms of insight and execution, and her energetic followup
        did much to move forward the events in this timeline.
        Hopper has a big reputation and it is fully deserved
 <a href="#footnote-11-ref">&#8617;</a></p>
<p id="footnote-12">12.
        Knuth and Pardo 1976,
        p. 50.
 <a href="#footnote-12-ref">&#8617;</a></p>
<p id="footnote-13">13.
        Knuth and Pardo 1976,
        p 42.
 <a href="#footnote-13-ref">&#8617;</a></p>
<p id="footnote-14">14.
        Knuth and Pardo 1976,
        pp. 42-49.
 <a href="#footnote-14-ref">&#8617;</a></p>
<p id="footnote-15">15.
        "http://www.softwarepreservation.org/projects/FORTRAN/paper/Backus-ProgrammingInAmerica-1976.pdf
        pp. 133-134
 <a href="#footnote-15-ref">&#8617;</a></p>
<p id="footnote-16">16.
        Knuth and Pardo 1976,
        pp. 83-86.
 <a href="#footnote-16-ref">&#8617;</a></p>
<p id="footnote-17">17.
        A.J. Perlis, J.W. Smith and H.R. vanZoeren,
        "Internal Translator (IT)
        A Compiler for the 650",
        Computation Center,
        Carnegie Institute of Technology,
        April 18, 1958
        pp 1.17-1.22
 <a href="#footnote-17-ref">&#8617;</a></p>
<p id="footnote-18">18.
        D.E. Knuth, “A History of Writing Compilers,”
        in
        _COMPUTERS and AUTOMATION_, December, 1962,
        pp. 8-10.
        1956 date is from
        "The FORTRAN I Compiler", David Padua,
        in
        Computing in Science & Engineering 2, pp. 70-75 (2000); https://doi.org/10.1109/5992.814661
 <a href="#footnote-18-ref">&#8617;</a></p>
<p id="footnote-19">19.
        J. Chipps, M. Koschmann, S. Orgel, A. Perlis, J. S, "A mathematical language compiler",
        in (1956) _Proceedings of the 1956 11th ACM national meeting_
 <a href="#footnote-19-ref">&#8617;</a></p>
<p id="footnote-20">20.
        Chomsky, Noam. "Three models for the description of language", IEEE Trans. Inform.
        Theory, 2(3):113–124, 1956.
 <a href="#footnote-20-ref">&#8617;</a></p>
<p id="footnote-21">21.
          Chomsky seems
          to have been unaware of Post's work -- he does not cite it.
 <a href="#footnote-21-ref">&#8617;</a></p>
<p id="footnote-22">22.
.
        Backus's notation is influenced by his study of Post --
        he seems not to have read Chomsky until later.
        [http://archive.computerhistory.org/resources/text/Oral_History/Backus_John/Backus_John_1.oral_history.2006.102657970.pdf],
        p. 25
 <a href="#footnote-22-ref">&#8617;</a></p>
<p id="footnote-23">23.
        As a pedantic point, a general parser need only parse
        <term>proper</term>
        grammars. A BNF grammar is
        <term>proper</term>
        if it has no useless rules and no infinite loops.
        Neither of these
        have any practical use, so that the restriction to proper grammars
        is unproblematic.
 <a href="#footnote-23-ref">&#8617;</a></p>
<p id="footnote-24">24.
        <a href="#bib-Glennie_1960">Glennie 1960</a>.
 <a href="#footnote-24-ref">&#8617;</a></p>
<p id="footnote-25">25.
        For the context-free BNF notation credits both Chomsky and Backus,
        and observes that the two notations are
        <term>related</term>.
        He also mentions
        Post's productions.
 <a href="#footnote-25-ref">&#8617;</a></p>
<p id="footnote-26">26.
        <a href="#bib-Glennie_1960">Glennie 1960</a>.
 <a href="#footnote-26-ref">&#8617;</a></p>
<p id="footnote-27">27.
        <a href="#bib-Glennie_1960">Glennie 1960</a>, Section 1.2, "A Notation for Syntax",
 <a href="#footnote-27-ref">&#8617;</a></p>
<p id="footnote-28">28.
        <a href="#bib-Samuelson_and_Bauer_1960">Samuelson and Bauer 1960</a>.
 <a href="#footnote-28-ref">&#8617;</a></p>
<p id="footnote-29">29.
        <a href="#bib-Irons_1961">Irons 1961</a>.
        Among those who state that
        <a href="#bib-Irons_1961">Irons 1961</a>
        parser is what
        is now called "left-corner" is Knuth ("Top-down syntax analysis", p. 109).
 <a href="#footnote-29-ref">&#8617;</a></p>
<p id="footnote-30">30.
        Irons is credited with the discovery of synthetic attributes
        by Knuth ("Genesis of Attibute Grammars").
 <a href="#footnote-30-ref">&#8617;</a></p>
<p id="footnote-31">31.
        <a href="#bib-Lucas_1961">Lucas 1961</a>.
        I follow
        <a href="#bib-Grune_and_Jacobs_2008">Grune and Jacobs 2008</a>
        in calling Lucas the discoverer of recursive descent.
        In fact, both
        <a href="#bib-Irons_1961">Irons 1961</a>
        and
        <a href="#bib-Lucas_1961">Lucas 1961</a>
        are recursive descent with a major bottom-up element.
        In conversation, Irons often described his 1961 parser as a kind of recursive descent.
        Perhaps Grune and Jacobs based their decision on the Lucas' description of his algorithm,
        which talks about his parser's bottom-up element only briefly,
        while describing the top-down element in detail.
        Also, the Lucas algorithm more resembles modern implementations of recursive descent
        much more closely than
        <a href="#bib-Irons_1961">Irons 1961</a>
 <a href="#footnote-31-ref">&#8617;</a></p>
<p id="footnote-32">32.
        As a former student of Irons at Yale,
        my personal preference would be to give Ned the credit
        for recursive descent.
        And this is what Peter Denning does in his introduction
        to the reprint of
        <a href="#bib-Irons_1961">Irons 1961</a>
        (<cite>Communications of the ACM</cite>, 25th Anniversary Issue, p. 14).
        But Grune and Jacobs 2008 gives the credit to Lucas 1961
 <a href="#footnote-32-ref">&#8617;</a></p>
<p id="footnote-33">33.
          Edsger W. Dijkstra, "Algol 60 translation : An Algol 60 translator
          for the x1 and Making a translator for Algol 60", Research Report 35,
          Mathematisch Centrum, Amsterdam, 1961,
 <a href="#footnote-33-ref">&#8617;</a></p>
<p id="footnote-34">34.
          <a href="#bib-Schorre_1964">Schorre 1964</a>, p. D1.3-1
 <a href="#footnote-34-ref">&#8617;</a></p>
<p id="footnote-35">35.
          Schorre 1964, p. D1.3-1
          http://ibm-1401.info/Meta-II-schorre.pdf
 <a href="#footnote-35-ref">&#8617;</a></p>
<p id="footnote-36">36.
          <a href="#bib-Knuth_1965_">Knuth 1965.</a>
          <footnote>.
            The LR algorithm is deterministic,
            Chomskyan and bottom-up. Knuth is primarily interested in the
            mathematics, and the parsing algorithm he gives is not practical. He
            leaves development of practical LR parsing algorithms as an
            <q>open question</q>
            for
            <q>future research</q><footnote>
              <a href="#bib-Knuth_1965">Knuth 1965</a>, p. 637
 <a href="#footnote-36-ref">&#8617;</a></p>
<p id="footnote-37">37.
          <a href="#bib-Lewis_and_Stearns_1968">Lewis and Stearns 1968</a>.
          They are credited in Rosencrantz and Stearns (1970)
          and
          <a href="#bib-Aho_and_Ullman_1972">Aho and Ullman 1972</a>, p. 368.
 <a href="#footnote-37-ref">&#8617;</a></p>
<p id="footnote-38">38.
          <a href="#bib-Knuth_1965">Knuth 1965</a>, p. 610.
          See on p. 611
          "corresponds with the intuitive notion of translation
          from left to right looking k characters ahead".
 <a href="#footnote-38-ref">&#8617;</a></p>
<p id="footnote-39">39.
          Knuth, "Top-down syntax analysis", p. 102.
          LL and LR have mirror images: RL means
          <q>scan from the right,
            using left reductions</q>
          and RR acquires its current meaning
          of
          <q>scan from the right, using right reductions</q>.
          Practical use of these
          mirror images is rare, but it may have occurred
          in one of the algorithms in our timeline
          --
          operator expression parsing
          in the IT compiler seems to have been RL(2) with backtracking.
 <a href="#footnote-39-ref">&#8617;</a></p>
<p id="footnote-40">40.
          <a href="#bib-Earley_1968_">Earley 1968.</a>
 <a href="#footnote-40-ref">&#8617;</a></p>
<p id="footnote-41">41.
          <a href="#bib-Knuth_1968">Knuth 1968</a>.
 <a href="#footnote-41-ref">&#8617;</a></p>
<p id="footnote-42">42.
          <a href="#bib-Aho_and_Ullman_1972">Aho and Ullman 1972</a>.
 <a href="#footnote-42-ref">&#8617;</a></p>
<p id="footnote-43">43.
          <a href="#bib-Aho_and_Ullman_1972">Aho and Ullman 1972</a>, p 321.
 <a href="#footnote-43-ref">&#8617;</a></p>
<p id="footnote-44">44.
          <a href="#bib-Aho_and_Ullman_1972">Aho and Ullman 1972</a>, pp. 456-485.
 <a href="#footnote-44-ref">&#8617;</a></p>
<p id="footnote-45">45.
              <a href="#bib-Aho_and_Ullman_1972">Aho and Ullman 1972</a>, p. 466.
 <a href="#footnote-45-ref">&#8617;</a></p>
<p id="footnote-46">46.
          http://www.wikiwand.com/en/Talk:Metacompiler/Archive_2
 <a href="#footnote-46-ref">&#8617;</a></p>
<p id="footnote-47">47.
          <a href="#bib-Samuelson_and_Bauer_1960">Samuelson and Bauer 1960</a>
          did not even include a BNF grammar --
          just the precedence tables.
 <a href="#footnote-47-ref">&#8617;</a></p>
<p id="footnote-48">48.
          <a href="#bib-Pratt_1973">Pratt 1973</a>.
 <a href="#footnote-48-ref">&#8617;</a></p>
<p id="footnote-49">49.
          <a href="#bib-Synder_1975">Synder 1975</a>.
          See, in particular, pp. 67-68.
 <a href="#footnote-49-ref">&#8617;</a></p>
<p id="footnote-50">50.
          <a href="#bib-Aho_and_Ullman_1977">Aho and Ullman 1977</a>.
 <a href="#footnote-50-ref">&#8617;</a></p>
<p id="footnote-51">51.
          <a href="#bib-McIlroy_and_Kernighan_1979">McIlroy and Kernighan 1979</a>.
 <a href="#footnote-51-ref">&#8617;</a></p>
<p id="footnote-52">52.
          S. C. Johnson, "Yet another compiler-compiler", in Volume 2A
          of
          <a href="#bib-McIlroy_and_Kernighan_1979">McIlroy and Kernighan 1979</a>.
 <a href="#footnote-52-ref">&#8617;</a></p>
<p id="footnote-53">53.
          Larry Wall, "v13i001: Perl, a "replacement" for awk and sed, Part01/10". Newsgroup: comp.sources.unix.
          1988-02-01).
          [https://groups.google.com/forum/#!topic/comp.sources.unix/Njx6b6TiZos].
 <a href="#footnote-53-ref">&#8617;</a></p>
<p id="footnote-54">54.
          Philip Wadler,
          "Comprehending Monads"
          In
          <cite>Proceedings of the 1990 ACM conference on LISP and functional programming</cite>
          (LFP '90).
          ACM, New York, NY, USA, 61-78. DOI=http://dx.doi.org/10.1145/91556.91592
 <a href="#footnote-54-ref">&#8617;</a></p>
<p id="footnote-55">55.
          Wadler 1985.
 <a href="#footnote-55-ref">&#8617;</a></p>
<p id="footnote-56">56.
          <a href="#bib-Leo_1991">Leo 1991</a>.
 <a href="#footnote-56-ref">&#8617;</a></p>
<p id="footnote-57">57.
          Perl 6 IRC, August 30, 2014.
          [https://irclog.perlgeek.de/perl6/2014-08-30#i_9271280]
 <a href="#footnote-57-ref">&#8617;</a></p>
<p id="footnote-58">58.
          Some source consider
          <a href="#bib-Wadler_1985">Wadler 1985</a>
          an
          early presentation of the ideas of combinator parsing.
          In assigning priority here,
          I follow Grune and Jacobs 2008 (p. 564).
 <a href="#footnote-58-ref">&#8617;</a></p>
<p id="footnote-59">59.
          The paper which is devoted to parsing is
          <a href="#bib-Hutton_1992">Hutton 1992</a>.
          The other paper, which centers on combinators as a programming
          paradigm, is
          Frost, Richard A. Constructing programs as executable attribute
          grammars. Computer J., 35(4):376–389, 1992.
          Frost only mentions parsing in one paragraph, and
          that focuses on implementation issues.
          Some of his grammars include operator expressions,
          but these avoid left recursion,
          and implement precedence and associativity,
          using parentheses.
 <a href="#footnote-59-ref">&#8617;</a></p>
<p id="footnote-60">60.
          <a href="#bib-Hutton_1992">Hutton 1992</a>, pp. 19-20.
 <a href="#footnote-60-ref">&#8617;</a></p>
<p id="footnote-61">61.
          Wadler Philip. (1995) Monads for functional programming. In:
          Jeuring J., Meijer E. (eds) Advanced Functional Programming. AFP
          1995. Lecture Notes in Computer Science, vol 925. Springer,
          Berlin, Heidelberg.
 <a href="#footnote-61-ref">&#8617;</a></p>
<p id="footnote-62">62.
          <a href="#bib-Hutton_and_Meijer_1996">Hutton and Meijer 1996</a>
 <a href="#footnote-62-ref">&#8617;</a></p>
<p id="footnote-63">63.
          Hudak, Peterson and Fasel,
          "A Gentle Introduction to Haskell 98",
          1999.
 <a href="#footnote-63-ref">&#8617;</a></p>
<p id="footnote-64">64.
          <a href="#bib-Aycock_and_Horspool_2002">Aycock and Horspool 2002</a>.
 <a href="#footnote-64-ref">&#8617;</a></p>
<p id="footnote-65">65.
          <a href="#bib-Ford_2004">Ford 2004</a>.
          See also
          <a href="#bib-Ford_2002">Ford 2002</a>.
 <a href="#footnote-65-ref">&#8617;</a></p>
<p id="footnote-66">66.
              "GCC 4.1 Release Series: Changes, New Features, and Fixes"
              [http://gcc.gnu.org/gcc-4.1/changes.html]
 <a href="#footnote-66-ref">&#8617;</a></p>
<p id="footnote-67">67.
          http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2010/06/marpa-is-now-on-for-right-recursions.html
 <a href="#footnote-67-ref">&#8617;</a></p>
<p id="footnote-68">68.
          http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2012/08/precedence-parsing-made-simpler.html
 <a href="#footnote-68-ref">&#8617;</a></p>
</body>
</html>

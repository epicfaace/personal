<!-- 
      Formatted using
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
  -->
<html>
  <head>
    <style type="text/css">
      mytitle { font-size: 300%; text-align: center; display: block }
      author { font-size: 200%; text-align: center; display: block }
      version { font-size: 200%; text-align: center; display: block }
      h1 { font-size: x-large }
      body { font-size: large }
      comment { display:none }
      footnote { display:none }
      biblio { display:none }
      bibref { font-weight: bold }
      bibid { font-weight: bold }
      term { }
      term::before { content: open-quote }
      term::after { content: close-quote }
    </style>
  </head>
  <body>
    <mytitle>Parsing: a timeline</mytitle>
    <version>Version 3.0</version>
    <author>Jeffrey Kegler</author>
    <h1>4th BCE: Pannini's description of Sanskrit</h1>
    <p>In India, Pannini creates an exact and complete description of
      the Sanskrit language, including pronunciation. Sanskrit could be
      recreated using nothing but Pannini's grammar.
      Pannini's grammar is
      probably the first formal system of any kind, predating Euclid. Even
      today, nothing like it exists for any other natural language of
      comparable size or corpus. Pannini is the object of serious study
      today. But in the 1940's and 1950's Pannini is almost unknown in
      the West. It will have no direct effect on the other events in
      this timeline.
    </p>
    <h1>1906: Markov's chains</h1>
    <p>Andrey Markov introduces his
      <term>chains</term>
      -- a set of
      states with transitions between them.
      One offshoot of Markov's work will be what we now
      know as regular expressions.
      Markov uses his chains, not
      for parsing, but for solving problems in probability<footnote>
        Andrey Andreyevich Markov, "Extension of the law of large numbers to quantities, depending on each other" (1906).
        The title is a translation -- the original in Russian.
      </footnote>.
    </p>
    <h1>1943: Post's rewriting system</h1>
    <p>Emil Post defines and studies a formal rewriting system<footnote>
        Emil L. Post,
        "Formal Reductions of the General Combinatorial Decision Problem",
        <cite>American Journal of Mathematics</cite>,
        Vol. 65, No. 2, pp. 197-215 (April 1943).
        DOI: 10.2307/2371809
      </footnote>
      using productions.
      With this, the process of rediscovering Pannini in the
      West begins.</p>
    <h1>1945: Turing discovers stacks</h1>
    <p>Alan Turing discovers the stack as part of his design of the
      ACE machine. This is important in parsing because recursive parsing
      requires stacks. The importance of Turing's discovery is not noticed
      at the time and stacks will be re-discovered many times over the
      next two decades<footnote>
        Carpenter, B. E.; Doran, R. W. (January 1977). "The other Turing machine". The Computer Journal. 20 (3): 269–279.
      </footnote>.
    </p>
    <h1>1948: Shannon repurposes Markov's chains</h1>
    <p>Claude Shannon publishes the foundation paper of information theory<footnote>
        <bibref>Shannon 1948</bibref>
      </footnote>.
      In this
      paper, Shannon makes an attempt to model English using Andrey Markov's
      chains<footnote>pp. 4-6.
        <bibref>Shannon 1948</bibref>, pp. 4-6.
      </footnote>.
    </p>
    <h1>1949: Rutishauser's compiler</h1>
    <p>From 1949 to 1951 at the ETH Zurich, Heinz Rutishauser worked on
      the design of what we would now call a compiler<footnote>
        <bibref>Knuth and Pardo 1976</bibref>,
        pp. 29-35, 40.
      </footnote>.
      Rutishauser's arithmetic expression parser
      did not honor precedence but it does allow nested parentheses.
      It is perhaps the first algorithm which can really be considered a
      parsing method.
      Rutishauser's compiler was never implemented.</p>
    <h1>The Operator Issue as of 1949</h1>
    <p>
      In the form of arithmetic expressions, operator expressions
      are the target of the first efforts at automatic parsing.
      Perhaps surprising, we will not see this issue go away.
    </p>
    <p>
      Very informally<footnote>
        The formal apparatus for describing operator expressions is not fully formed
        as of 1949 and,
        in any case,
        a very formal definition is beyond the scope of this timeline.
      </footnote>,
      we can say that
      an operator expression is an expression
      built up from operands
      and operators.
      It is expected that the operand might be another operator expression,
      so operator expressions raise the issue of recursion.
    </p>
    <p>
      The archetypal examples of operator expressions are arithmetic expressions:
    </p><pre><tt>
         2+(3*4)
         13^2^-5*9/11
         1729-42*8675309
      </tt></pre>
    <p>
      In Western mathematics arithmetic expressions were read
      according to traditional<footnote>
      </footnote>
      ideas of
      associativity and precedence:
    </p><ul>
      <li><tt>^</tt>
        is exponentiation.
        It right associates and has tightest<footnote>
          This timeline refers to precedence levels as
          <q>tight</q>
          or
          <q>loose</q>.
          The traditional terminology is confusing: tighter
          precedences are
          <q>higher</q>
          but traditionally the
          precendence levels are also numbered and the higher
          the number, the lower the precedence.
        </footnote>
        precedence.
      </li>
      <li>
        Multiplication (<tt>*</tt>) and division (<tt>/</tt>) left associate.
        They have a precedence equal to each other
        and less tight than that of exponentiation.
      </li>
      <li>
        Addition ('+') and subtraction ('-') left associate.
        They have a precedence equal to each other
        and less tight than that of multiplication and division.
      </li>
      <li>
        Parentheses, when present, override the traditional
        associativity and precedence.
      </li>
    </ul>
    <p>Certainly in the first attempts at parsing in compilers,
      arithmetic expressions are central.
      The first compiled languages are organized line-by-line and,
      except for arithmetic expressions,
      lines are
      interpreted using basic string manipulations.
      It is only when those string manipulations turn to dealing
      with arithmetic expressions that they become sophisticated
      enough to be called parsing techniques,
      and only then that
      having a parsing theory to describe them becomes helpful.
    </p>
    <p>
      Rutishauser's language, and in fact all languages before
      LISP,
      is structured line-by-line.
      No language before ALGOL is truly block-structured.
    </p>
    <p>
      The line-by-line languages
      are parsed using string manipulations.
      A parsing theory is not helpful for describing these
      ad hoc string manipulations,
      so they don't give rise to one.
      The only logic in these early compilers that really deserves to be called
      a parsing method,
      is that which tackles arithmetic expressions.
    </p><p>
    </p>
    <h1>1950: Boehm's compiler</h1>
    <p>During 1950, Corrado Boehm, also at the ETH Zurich develops his
      own compiler.
      Rutishauser and Boehm are working at the same institution at the same
      time, but Boehm is unaware of Rutishauser's work until his own is
      complete.
      Boehm's is also the first self-compiling compiler -- it is written
      in its own language.
    </p>
    <p>
      Like Rutishauser, Boehm's language is line-by-line and
      parsed ad hoc, except for expressions. Boehm's expression parser
      <em>does</em>
      honor precedence, making it perhaps the first operator precedence
      parser<footnote>
        Many of the
        <q>firsts</q>
        for parsing algorithms in this
        timeline are debatable,
        and the history of operator precedence is especially murky.
        Here I follow
        <bibref>Samuelson and Bauer 1960</bibref>
        in giving the priority to Boehm.
      </footnote>.
      Boehm's compiler also allows parentheses, but the two cannot
      be mixed -- an expression can either be parsed using precedence
      or have parentheses, but not both.
      And like Rutishauser's, Boehm's compiler is never
      implemented<footnote>
        <bibref>Knuth and Pardo 1976</bibref>,
        pp 35-42.
      </footnote>.
    </p>
    <h1>1952: Grace Hopper uses the term
      <term>compiler</term></h1>
    <p>Grace Hopper writes a linker-loader.
      <a
      href="https://en.wikipedia.org/wiki/History_of_compiler_construction#First_compilers%22">She
        calls it a
        <term>compiler</term></a>. Hopper seems to be the first
      person to use this term for a computer program.</p>
    <h1>Term:
      <term>Compiler</term>
      as of 1952</h1>
    <p>Hopper used the term
      <term>compiler</term>
      in a meaning it
      had at the time:
      <q>to compose out of materials from other
        documents</q><footnote>
        Quoted definition is from Nora B. Moser,
        "Compiler method of automatic programming",
        Symposium on Automatic Programming for Digital Computer,
        ONR, p. 15.,
        as cited in
        <bibref>Knuth and Pardo 1976</bibref>,
        p 51.
      </footnote>.
      Specifically, in 1952,
      subroutines were new,
      and automated programming
      (what we would come to call
      <q>compiling</q>)
      often was viewed as providing a interface
      for calling a collection of
      carefully chosen assembler subroutines<footnote>
        In 1952, an interface that guided calls to subroutines
        was much more helpful than current programmers might
        imagine:
        "Existing programs for similar problems were unreadable
        and hence could not be adapted new uses."
        (<bibref>Backus 1980</bibref>, p. 126)
      </footnote>.
      Hopper's new
      program took this subroutine calling
      one step further -- instead of calling the subroutines
      it expanded them (or in Hopper's terms
      <q>compiled</q>
      them) into
      a single program.
      Since Hopper the term
      <term>compiler</term>
      has acquired a
      more specialized and somewhat different meaning in the computer field.
      Today we would
      not call Hopper's program a
      <term>compiler</term><footnote>
        I hope nobody will read this terminological clarification as in any sense
        detracting from Hopper's achievement.
        Whatever it is called, Hopper's program was a major advance,
        both in terms of insight and execution, and her energetic followup
        did much to move forward the events in this timeline.
        Hopper has a big reputation and it is fully deserved</footnote>.
    </p>
    <p>As we have seen, programs that today we
      <em>would</em>
      call compilers
      were envisioned before Hopper,
      but nobody called these programs compilers -- compiling in our modern sense
      was called
      <q>automatic coding</q>,
      <q>codification automatique</q>
      or
      <q>Rechenplanfertigung</q><footnote>
        <bibref>Knuth and Pardo 1976</bibref>,
        p. 50.
      </footnote>.
    </p>
    <h1>1951: Kleene's regular languages</h1>
    <p>Kleene discovers regular languages<footnote>
        <bibref>Kleene 1951</bibref>.
      </footnote>.
      Kleene does not use regular expression notation,
      but his regular languages are the idea behind
      it.
    </p>
    <h1>1952: Glennie's AUTOCODE</h1>
    <p>Knuth
      calls Glennie's the first
      <q>real</q><footnote>
        <bibref>Knuth and Pardo 1976</bibref>,
        p 42.
      </footnote>
      compiler, by which he means that
      it was actually implemented and used by someone to translate
      algebraic statements into machine language.
      Glennie's AUTOCODE
      was very close to the machine -- just above machine language.
      It did not allow operator expressions.
      AUTOCODE was also hard-to-use,
      and apparently saw little use by anybody but
      Glennie himself.
      Because
      Glennie worked for the British atomic weapons projects his papers
      were routinely classified,
      so even the indirect influence of AUTOCODE was
      slow to spread.
      Nonetheless, many other
      <term>compilers</term>
      afterward were named AUTOCODE, which probably indicates
      awareness of Glennie's effort<footnote>
        <bibref>Knuth and Pardo 1976</bibref>,
        pp. 42-49.
      </footnote>.
    </p>
    <h1>1954: The FORTRAN project begins</h1>
    <p>At IBM, a team under John Backus begins working on the language which will be called FORTRAN.</p>
    <h1><term>Compiler</term>
      as of 1954</h1>
    <p>The term
      <term>compiler</term>
      is still being used
      in Hopper's looser sense, instead of its modern, specialized,
      one.
      In particular, there was no implication that the output of a
      <term>compiler</term>
      is ready for execution by a computer.
      The output of one 1954
      <term>compiler</term>, for example, produced
      relative addresses, which needed to be translated by hand before
      a machine can execute them<footnote>
        <bibid>Backus 1980</bibid>, pp. 133-134.
      </footnote>.</p>
    <h1>1955: Noam Chomsky starts teaching at MIT</h1>
    <p>Noam Chomsky is awarded a Ph.D. in linguistics and accepts a
      teaching post at MIT. MIT does not have a linguistics department and
      Chomsky is free to teach his own approach,
      Chomksy's linguistics course is
      highly original and very mathematical.</p>
    <h1>1955: Work begins on the IT compiler</h1>
    <p>At Purdue, a team including Alan Perlis and Joseph Smith begins work
      on the IT compiler<footnote>
        <bibref>Knuth and Pardo 1976</bibref>,
        pp. 83-86.
      </footnote>.
    </p>
    <h1>1956: The IT compiler is released</h1>
    <p>Perlis and Smith, now at the Carnegie Institute of Technology,
      finish the IT compiler. Don Knuth calls this
    </p><blockquote>
      the first really
      <em>useful</em>
      compiler. IT and IT's derivatives were used successfully and
      frequently in hundreds of computer installations until [its
      target] the [IBM] 650 became obsolete. [... P]previous systems
      were important steps along the way, but none of them had the
      combination of powerful language and adequate implementation and
      documentation needed to make a significant impact in the use of
      machines.<footnote>
        A.J. Perlis, J.W. Smith and H.R. vanZoeren,
        "Internal Translator (IT)
        A Compiler for the 650",
        Computation Center,
        Carnegie Institute of Technology,
        April 18, 1958
        pp 1.17-1.22
      </footnote>.
    </blockquote>
    <p>The IT language had arithmetic expressions, of a sort --
      parentheses are honored, but otherwise evaluation is always
      right-to-left -- there is no operator precedence. IT did honor
      parentheses, but nonetheless its way of doing arithmetic expressions
      proves very unpopular: Donald Knuth reports that
      <q>The lack of
        operator priority (often called precedence or hierarchy) in the IT
        language was the most frequent single cause of errors by the users
        of that compiler.</q><footnote>
        D.E. Knuth, “A History of Writing Compilers,”
        in
        _COMPUTERS and AUTOMATION_, December, 1962,
        pp. 8-10.
        1956 date is from
        "The FORTRAN I Compiler", David Padua,
        in
        Computing in Science & Engineering 2, pp. 70-75 (2000); https://doi.org/10.1109/5992.814661
      </footnote>
    </p>
    <h1><term>Compiler</term>
      as of 1956</h1>
    <p>In the 1956 document describing the IT compiler<footnote>
        J. Chipps, M. Koschmann, S. Orgel, A. Perlis, J. S, "A mathematical language compiler",
        in (1956) _Proceedings of the 1956 11th ACM national meeting_
      </footnote>,
      the IT team is careful to define the term.
      Their definition makes clear that
      they are using of the word
      <term>compiler</term>
      in something like its
      modern sense, perhaps for the first time. From this time on, when used
      as a technical term within computing,
      <term>compiler</term>
      will usually
      mean what we currently understand it to mean.</p>
    <h1>1956: The Chomsky hierarchy</h1>
    <p>Chomsky publishes the paper which is usually considered the foundation of
      Western formal language theory<footnote>
        <bibref>Chomsky 1956</bibref>.
      </footnote>.
      Chomsky demolishes the idea that natural language grammar can be
      modeled using only Markov chains. Instead, the paper advocates a
      natural language approach that uses three layers:</p>
    <ul>
      <li>Chomsky uses
        Markov's chains
        as his
        <b>bottom layer</b>.
        This becomes the modern compiler's
        <b>lexical phase</b>.</li>
      <li>Chomsky's
        <b>middle layer</b>
        uses context-free
        grammars and context-sensitive grammars.
        These are his own
        discoveries<footnote>
          Chomsky seems
          to have been unaware of Post's work -- he does not cite it.
        </footnote>.
        This middle layer becomes the
        <b>syntactic phase</b>
        of modern
        compilers.</li>
      <li>Chomsky's
        <b>top layer</b>, again his own
        discovery, maps or
        <term>transforms</term>
        the output of the middle
        layer. Chomsky's top layer is the inspiration for AST transformation
        phase of modern compilers.</li>
    </ul>
    <h1>Term:
      <term>Parser</term></h1>
    <p>Chomsky is a turning point, so much so that it establishes or settles the
      meaning of many of the terms we are using.
      A
      <term>Parser</term>,
      for our purposes, is something or someone that transforms
      a string of symbols into a
      structure,
      according to a description
      of the mapping from strings to structures.
      For our purposes,
      the structures can usually be considered to be parse trees.</p>
    <h1>Term:
      <term>Recognizer</term></h1>
    <p>In contrast to a parser,
      a
      <term>recognizer</term>
      is a something or someone that
      takes a string and answers
      <q>yes</q>
      or
      <q>no</q>
      --
      <q>yes</q>
      if the string is in the language described by the recognizer,
      <q>no</q>
      otherwise.
      Note that,
      if we intend to do semantics,
      a recognizer alone is not sufficient.
    </p>
    <p>In the strict sense, a recognizer cannot driver a compiler --
      a compiler needs a parser.
      But recognizers can be far easier to write,
      and are often hacked up
      and pressed into service as parsers.
      by introducing captures.
      For example, if your semantics is simple and non-recursive,
      you might be able to drive your semantics phase with
      the output of
      a regex engine, using captures.
    </p>
    <h1>1957: Chomsky publishes
      <cite>Syntactic Structures</cite></h1>
    <p>Noam Chomsky publishes
      <cite>Syntactic Structures</cite><footnote>
        <bibref>Chomsky 1957</bibref>.
      </footnote>, one of the most important books of
      all time.
      The orthodoxy in 1957 is structural linguistics which
      argues, with Sherlock Holmes, that
      <q>it is a capital mistake
        to theorize in advance of the facts</q>.
      Structuralists start
      with the utterances in a language, and build upward.</p>
    <p>But Chomsky claims that without a theory there are no facts: there
      is only noise. The Chomskyan approach is to start with a grammar,
      and use the corpus of the language to check its accuracy. Chomsky's
      approach will soon come to dominate linguistics.</p>
    <h1>Term:
      <term>Chomskyan parsing</term></h1>
    <p>From here on,
      all parsing theory is defined in terms of
      Chomsky's ideas.
      Parsing theory divides into Chomskyan and
      non-Chomskyan.
      Chomskyan parsing theory becomes and remains the mainstream
      but it is far from unchallenged.
    </p><p>
      Above we defined a
      <term>parser</term>
      (<term>recognizer</term>)
      as something or someone that
      parses (recognizes) a string
      according to a description.
      We can now be more specific.
      In Chomskyan parsing (recognizing),
      the description is that of Chomsky's middle layer.
      If the description is anything else,
      the parser (recognizer) is non-Chomskyan.
    </p>
    <p>
      As of 1957, we are calling the description
      of a Chomskyan middle layer,
      a
      <term>context-free grammar</term>.
      The BNF notation for context-free grammars
      has not yet been discovered.
      Once it is,
      we will refer to context-free grammars
      as BNF grammars.
    </p>
    <h1>1957: FORTRAN released</h1>
    <p>Backus's team makes the first FORTRAN compiler available to IBM
      customers. FORTRAN is the first high-level language that will find
      widespread implementation. As of this writing, it is the oldest
      language that survives in practical use.</p>
    <p>FORTRAN is a line-by-line language.
      Its parsing is non-Chomskyan.
      But it includes one important discovery.
      FORTRAN I was allowed expressions.
      And, learning from the
      dissatisfaction with the IT compiler,
      FORTRAN honors associativity
      and precedence.</p>
    <p>The designers of FORTRAN used a strange trick for parsing operator expresssions --
      they hacked the expressions by adding parentheses around each
      operator.
      The number of parentheses varied, depending on the operator.
      Surprisingly, this works.
      In fact, once the theoretical
      understanding of operator precedence comes about,
      the FORTRAN
      I implementation is recognized
      as a hackish and inefficient way of
      implementing the classic operator precedence algorithm.</p>
    <h1>1958: LISP released</h1>
    <p>John McCarthy's LISP appears. LISP goes beyond the line-by-line
      syntax -- it is recursively structured. But the LISP interpreter does
      not find the recursive structure: the programmer must explicitly
      indicate the structure herself, using parentheses.
      Similarly, LISP does not have operator expressions in the usual sense --
      associativity and precedence must be specified with parentheses.
    </p>
    <h1>1959: Backus's notation</h1>
    <p>Backus discovers a new notation to describe the IAL language<footnote>
        Backus's notation is influenced by his study of Post --
        he seems not to have read Chomsky until later.
        [http://archive.computerhistory.org/resources/text/Oral_History/Backus_John/Backus_John_1.oral_history.2006.102657970.pdf],
        p. 25.
        <bibref>Backus 1980</bibref>, p. 133.
      </footnote>.
      According to Backus's recollection,
      his paper<footnote>
        <bibref>Backus 1959</bibref>
      </footnote>
      has only one reader: Peter Naur<footnote>
        <bibref>Backus 1980</bibref>, pp. 132-133.
      </footnote>.
      The IAL language will soon be renamed ALGOL.
    </p>
    <h1>1959: Operator precedence and stacks</h1>
    <p>Samuelson and Bauer<footnote>
        <bibref>Samuelson and Bauer 1960</bibref>.
      </footnote>
      describe in detail the use of stacks to implement
      operator precedence.
      The algorithm and its presentation are thoroughly
      non-Chomskyan --
      Samuelson and Bauer describe their example using,
      not a context-free grammar,
      but an operator precedence table.
    </p><h1>The Operator Issue as of 1959</h1>
    <p>Since Boehm,
      many people have been refining operator precedence.
      With Samuelson and Bauer,
      what Norvell<footnote>
        <bibref>Norvell 1999</bibref>
      </footnote>
      calls "the classic algorithm"
      takes a well-documented form.
      The Samuelson and Bauer paper was very influential.
    </p>
    <h1>1960: The ALGOL report</h1>
    <p>The ALGOL 60 report<footnote>
        <bibref>ALGOL 1960</bibref>
      </footnote>
      specifies, for the first time, a block
      structured language. ALGOL 60 is recursively structured but the
      structure is implicit -- newlines are not semantically significant,
      and parentheses indicate syntax only in a few specific cases. The
      ALGOL compiler will have to find the structure.
    </p>
    <p>With the ALGOL 60 report, a quest begins which continues to this day: the search for a parser that is</p>
    <ul id="loc-quest">
      <li>efficient,</li>
      <li>practical,</li>
      <li>declarative, and</li>
      <li>general.</li>
    </ul>
    <p>
      It is a case of 1960's
      optimism at its best. As the ALGOL committee is well aware, a parsing
      algorithm capable of handling ALGOL 60 does not yet exist. But the
      risk they are taking has immediate results --
      1961 will see a number of discoveries that remain important
      today.
      On the other hand, the parser they seek remains elusive for
      decades.
    </p>
    <h1>1960: BNF</h1>
    <p>In the ALGOL 60 report<footnote>
        <bibref>ALGOL 60</bibref>
      </footnote>,
      Peter Naur improves the Backus notation and uses it to describe
      the language.
      This bring Backus' notation to wide attention.
      The improved notation will become known as Backus-Naur Form
      (BNF).
    </p>
    <h1>Term:
      <term>declarative</term></h1>
    <p>For our purposes, a parser is
      <term>declarative</term>,
      if it
      will parse directly and automatically from grammars written in BNF.
      Declarative parsers are often called
      <term>syntax-driven</term>
      parsers.
    </p>
    <h1>Term:
      <term>procedural</term></h1>
    <p>A parser is
      <term>procedural</term>, if it requires procedural
      logic as part of its syntax phase.
    </p>
    <h1>Term:
      <term>general</term></h1>
    <p>A general parser is a parser that will parse
      any grammar that can be written in BNF<footnote>
        As a pedantic point, a general parser need only parse
        <term>proper</term>
        grammars. A BNF grammar is
        <term>proper</term>
        if it has no useless rules and no infinite loops.
        Neither of these
        have any practical use, so that the restriction to proper grammars
        is unproblematic.
      </footnote>.
      languages.</p>
    <h1>1960: Gleenie's compiler-compiler</h1>
    <p>The first description of a consciously non-Chomskyan compiler
      seems to predate the first description of a Chomskyan parser.
      It is A.E. Gleenie's 1960 description of his compiler-compiler<footnote>
        <bibref>Glennie 1960</bibref>.
      </footnote>.
      Glennie's
      <term>universal compiler</term>
      apparently was useable,
      but it is more of a methodology
      than an implementation -- the compilers must be written by
      hand.
    </p>
    <p>
      Glennie uses BNF,
      but he does
      <em>note</em>
      use it in the way
      Chomsky, Backus and Naur intended BNF.
      Glennie is using BNF to describe a
      <b>procedure</b>.
      In true BNF, the order of the rules does not matter --
      in Glennie's pseudo-BNF order matters very much.
      This means that, for most practical grammars,
      Glennie's pseudo-BNF and BNF proper do
      <b>not</b>
      describe the same language.
      As we shall see,
      determining what language Glennie's pseudo-BNF does
      describe can be a very difficult matter.
    </p>
    <p>
      Glennie is well aware of what he his doing, and
      is not attempting to deceive anyone --
      he clearly points out that the distinction
      between his procedural pseudo-BNF and declarative BNF,
      and warns his reader that the difference is
      <q>important</q><footnote>
        <bibref>Glennie 1960</bibref>.
      </footnote>.
      <comment>
        Is Glennie's non-Chomskyan-ism on theoretical grounds,
        or pragmatic?
      </comment>
    </p>
    <h1>1961: The first parsing paper</h1>
    <p>In January, Ned Irons publishes a paper describing his ALGOL
      60 parser.
      It is the first paper to fully describe any parser.
      The Irons algorithm is pure Chomskyan
      and top-down with a bottom-up
      <term>left
        corner</term>
      element -- it is what now would be called a
      <term>left
        corner</term>
      parser.<footnote>
        <bibref>Irons 1961</bibref>.
        Among those who state that
        <bibref>Irons 1961</bibref>
        parser is what
        is now called
        <term>left-corner</term>
        is Knuth (<bibref>Knuth 1971</bibref>, p. 109).
      </footnote>
    </p>
    <p>You would expect parsing might start with hard-written
      parsers for specific grammar classes.
      But in fact, the Irons parser is declarative and general.
      And since it is general,
      operator expressions are within the power of
      the Irons parser<footnote>
        Although it seems likely that parsing operator expressions would require
        backtracking,
        and therefore could be inefficient.
      </footnote>.
    </p>
    <h1>Terms:
      <term>Top-down</term></h1>
    <p>A top-down parser deduces the consituents of a rule from the rule.
      That is, it looks at the rule first,
      and then deduces what is on its RHS.
      Thus, it works from start rule, and works
      <q>top down</q>
      until it arrives at the input tokens.
    </p>
    <p>It is important to note that no useful parser can be purely top-down --
      if parser worked purely pure top-down, it would never look at its input.
      So every top-parser we will consider has *some* kind of bottom-up element.
      That bottom-up element may be very simple -- for example, one character lookahead.
      The
      <bibref>Irons 1961</bibref>
      parser,
      like most modern top-down parsers,
      has a sophisticated bottom-up element.
    </p>
    <h1>Terms:
      <term>Bottom-up</term></h1>
    <p>A bottom-up parser deduces a rule from its constituents.
      That is, it looks at either the input or the LHS symbols
      of previously deduced rules,
      and from that deduces a rule.
      Thus, it works from the input tokens, and works
      <q>bottom up</q>
      until it reaches the start rule.
      A parser can be be purely bottom-up,
      but for efficiency it is usually best to eliminate potential
      parses based on the grammar,
      and that requires some kind of top-down logic.
    </p>
    <h1>Terms:
      <term>Synthetic attribute</term></h1>
    <p><bibref>Irons 1961</bibref>
      also introduces synthetic attributes: the parse creates
      a tree, which is evaluated bottom-up. Each node is evaluated using
      attributes
      <q>synthesized</q>
      from its child nodes.<footnote>
        Irons is credited with the discovery of synthetic attributes
        by Knuth ("Genesis of Attibute Grammars").
      </footnote>
      Synthetic attributes are a concept in semantics, not parsing,
      but they will be important for us.
    </p>
    <h1>1961: Lucas discovers recursive descent</h1>
    <p>Peter Lucas publishes the first description of a purely top-down parser<footnote>
        <bibref>Lucas 1961</bibref>.
        I follow
        <bibref>Grune and Jacobs 2008</bibref>
        in calling Lucas the discoverer of recursive descent.
        In fact, both
        <bibref>Irons 1961</bibref>
        and
        <bibref>Lucas 1961</bibref>
        are recursive descent with a major bottom-up element.
        In conversation, Irons often described his 1961 parser as a kind of recursive descent.
        Perhaps Grune and Jacobs based their decision on the Lucas' description of his algorithm,
        which talks about his parser's bottom-up element only briefly,
        while describing the top-down element in detail.
        Also, the Lucas algorithm more resembles modern implementations of recursive descent
        much more closely than
        <bibref>Irons 1961</bibref></footnote>.
      Either Irons paper or this one can be considered to be recursive descent<footnote>
        As a former student of Irons at Yale,
        my personal preference would be to give Ned the credit
        for recursive descent.
        And this is what Peter Denning does in his introduction
        to the reprint of
        <bibref>Irons 1961</bibref>
        (<cite>Communications of the ACM</cite>, 25th Anniversary Issue, p. 14).
        But Grune and Jacobs 2008 gives the credit to Lucas 1961
      </footnote>.
      Except to say that he deals properly with them, Lucas does not say
      how he parses operator expressions.
      But it is easy to believe Lucas'
      claim -- by this time the techniques for
      parsing operator expressions are well understood<footnote>
        <bibref>Lucas 1961</bibref>
        cites
        <bibref>Samuelson and Bauer 1960</bibref>
      </footnote>.
    </p>
    <h1>1961: Dijkstra's shunting yard algorithm</h1>
    <p>In November 1961, Dijkstra publishes the
      <q>shunting yard</q>
      algorithm.<footnote>
        Edsger W. Dijkstra, "Algol 60 translation : An Algol 60 translator
        for the x1 and Making a translator for Algol 60", Research Report 35,
        Mathematisch Centrum, Amsterdam, 1961,
      </footnote>
      In Norvell's useful classification<footnote>
        <bibref>Norvell 1999</bibref>.
      </footnote>
      of operator expression parsers,
      All
      earlier parsers have been what he calls
      <q>the classic
        algorithm</q>.
      Dijkstra's approach is new.
      In the classic approach,
      the number
      of level of precedence is hard-coded into the
      algorithm.
      Dijkstra's algorithm can handle any number of levels
      of precedence without a change in its code,
      and without any effect on its running speed.
    </p>
    <h1>The Operator Issue as of 1961</h1>
    <p>The results of 1961 transformed the Operator Issue.
      Before ALGOL,
      parsing operator expression parsing essentially
      <em>was</em>
      parsing.
      After ALGOL, almost all languages will be block-structured
      and ad hoc string manipulatons are no longer adequate --
      the language as a whole requires a serious parsing technique.
      So parsing operator expressions becomes a side show,
      or so it seems.
    </p>
    <p>
      Why not use the
      algorithms that parse operator expressions for the whole
      language?
      <bibref>Samuelson and Bauer 1959</bibref>
      had suggested
      exactly that.
      But, alas, operator expression parsing is not adequate for
      languages as a whole<footnote>
        But see the entry for 1973 on Pratt parsing
        (<bibref>Pratt 1973></bibref>)
        where the idea of parsing
        entire languages as operator grammars is revisited
      </footnote>.
    </p>
    <p>Also by 1961, we have BNF.
      This gives us a useful notation
      for describing grammars.
      It allows us to introduce our Basic Operator Grammar (BASIC-OP):
    </p>
    <ul id="g-basic-op">
      <li>S ::= E</li>
      <li>E ::= E + T</li>
      <li>E ::= T</li>
      <li>T ::= T * F</li>
      <li>T ::= F</li>
      <li>F ::= number</li>
    </ul>
    <p><a href="#g-basic-op">BASIC-OP</a>
      has two operators, three levels of precedence
      and left associativity.
      This was enough to challenge the primitive parsing techniques in use
      before 1961.
      Surprisingly, this simple grammar will bedevil mainstream parsing theory for the
      next half a century.
    </p><p>
      Recursive descent, it turns out,
      cannot parse
      <a href="#g-basic-op">BASIC-OP</a>
      because it is left recursive.
      And that is not the end of it.
      Making addition and multiplication right-associate
      is unnatural and,
      as the authors of the IT compiler found out,
      causes users to revolt.
      But suppose we try to use this
      Right-recursive Operator Grammar (RIGHT-OP)
      anyway:
    </p>
    <ul id="g-right-op">
      <li>S ::= E</li>
      <li>E ::= T + E</li>
      <li>E ::= T</li>
      <li>T ::= F * T</li>
      <li>T ::= F</li>
      <li>F ::= number</li>
    </ul>
    <p>
      Recursive descent, without help,
      cannot parse
      <a href="#g-right-op">RIGHT-OP</a>.
      Parsing theory has not developed well enough to
      state why in a precise terms.
      Suffice it to say for now
      that
      <a href="#g-rr">RIGHT-OP</a>
      requires too much lookahead.
    </p>
    <p>But recursive descent does have a huge advantage,
      one which, despite its severe limitations,
      will save it from obsolescence time and again.
      Hand-written recursive descent is essentially calling
      subroutines.
      Adding custom modification to recursive descent
      is very straight-forward.
    </p>
    <p>
      In addition,
      while pure recursive descent cannot
      <em>parse</em>
      operator expressions,
      but it can
      <em>recognize</em>
      them.
      This means pure recursive descent may not be able to create
      the parse subtree for an operator expression itself,
      but it can recognize the expression and hand control
      over to a specialized operator expression parser.
      This seems to be what Lucas' 1961 algorithm did,
      and it is certainly what many other implementations did afterwards.
      Adding the operator expression subparser makes the implementation
      only quasi-Chomskyan,
      but this was a price the profession has
      been willing to pay.
    </p>
    <p>Alternatively,
      a recursive descent implementation can parse operator expressions
      as lists,
      and add associativity in post-processing.
      This pushes some of the more important parsing
      out of the syntactic phase into the semantics
      but, again,
      it seemed that,
      if the ship was to stay afloat,
      Chomskyan purity had to be thrown overboard.
    </p>
    <p>Bottom line: as of 1961 the operator issue takes a new form.
      Because of the operator issue,
      recursive descent is not
      sufficient for practical grammars -- it must always be part of a
      hybrid.
    </p>
    <p>In this context,
      Dijkstra's new 1961 algorithm is a welcome
      alternative choice: as a operator expression subparser,
      it can parse operator expressions faster and in less space.
      But Dijkstra's algorithm has no more parsing power
      than the classic operator
      precedence algorithm --
      it did nothing to change the basic tradeoffs.
    </p>
    <h1>1964: The Meta II compiler</h1>
    <p>Schorre publishes a paper on the Meta II
      <q>compiler
        writing language</q>, summarizing the papers of the 1963
      conference. Schorre cites both Backus and Chomsky as sources for
      Meta II's notation.
      Schorre notes that his parser is
      <q>entirely different</q>
      from that of
      <bibref>Irons 1961</bibref>
      --
      in fact, it is non-Chomskyan.
      Meta II is a template, rather than something that
      his readers can use, but in principle it can be turned into a fully
      automated compiler-compiler<footnote>
        <bibref>Schorre 1964</bibref>, p. D1.3-1
      </footnote>.
    </p>
    <h1>1965: Knuth discovers LR</h1>
    <p>Don Knuth discovers LR parsing<footnote>
        <bibref>Knuth 1965.</bibref>
      </footnote>.
      The LR algorithm is deterministic,
      Chomskyan and bottom-up. Knuth is primarily interested in the
      mathematics, and the parsing algorithm he gives is not practical. He
      leaves development of practical LR parsing algorithms as an
      <q>open question</q>
      for
      <q>future research</q><footnote>
        <bibref>Knuth 1965</bibref>, p. 637
      </footnote>.
    </p>
    <h1>Term:
      <term>linear</term></h1><p>
      Among
      <a href="#loc-quest">the goals</a>
      for the ideal
      ALGOL parser was that it be
      <q>efficient</q>.
      As of 1965 this notion has become more precise,
      thanks to a notation that Knuth borrowed from
      calculus.
      This
      <q>big O</q>
      notation characterizes algorithm's
      using functions, but it treats functions that
      differ by a constant multiple as identical<footnote>
        This timeline is not a mathematics tutorial,
        and I have ignored important complications to
        avoid digression.
        A reader who does not know the details,
        they can be
        <a href="https://en.wikipedia.org/wiki/Big_O_notation">worth learning</a>.
      </footnote>.
      <q>Ignoring the constant</q>
      means that conclusions
      drawn from
      <q>big O</q>
      results outlast steady improvements
      in technology,
      but capture non-linear jumps.
    </p>
    <p>
      These big O functions take as input some aspect of the algorithms
      input.
      In the case of parsing, by convention,
      the big O function is usually of the length of the input<footnote>
        Because constants are ignored, all reasonable measures of
        the length are equivalent for
        <q>big O</q>
        notation.
        Some papers on parsing also consider the size of the grammar,
        but usually size of the grammar is regarded as a fixed constant,
        and in this timeline
        <q>big O</q>
        results will be reported only
        in terms of the input length.
      </footnote>.
    </p><p>Of the many possible
      <q>big O</q>
      functions,
      only a few will be of interest in this timeline.
      A function which grows steadily as the input grows
      is called
      <term>linear</term>.
      In
      <q>big O</q>
      notation,
      <term>linear</term>
      is
      written O(n).
      A function which grows as the length of the input times
      its logarithm is almost linear:
      <term>quasi-linear</term>.
      In
      <q>big O</q>
      notation,
      <term>quasi-linear</term>
      can be
      written O(n*log n).
      A function which grows as the square of the length
      is called
      <q>quadratic</q>
      -- O(n**2).
      A function which grows as the cube of the length
      is called
      <q>cubic</q>
      -- O(n**3).
      In extreme cases,
      a function can grow as a constant taken to the power
      of the length, or
      <term>exponentially</term>
      --
      O(c**n).
    </p><p>
      By this time,
      <q>efficient</q>
      for a parser means
      <q>linear</q>
      or
      <q>quasi-linear</q>.
      Parsers which operate in quadratic or cubic time are
      considered impractical.
      But parsers aimed at practitioners will often push the
      envelope --
      any parser which uses backtracking in potentially expoential
      and is designed in the hope that the backtracking will not
      get out of hand for your particular grammar.
    </p>
    <h1>Parsing LR(k): an influential misconception</h1>
    <p>In his 1965, Knuth makes
      a reasonable assumption,
      but one which turns out to be wrong:
    </p>
    <blockquote>
      Another important area of research is to develop algorithms that
      accept LR(k) grammars, or special classes of them, and to mechanically
      produce efficient parsing programs.
    </blockquote>
    <p>Implied in this is
      the belief that
      that better algorithms for parsing LR(k)
      grammars
      will either be parsers aimed specifically at LR(k)
      or at subsets of LR(k).
      That is, Knuth excludes the idea that algorithms which
      target
      <b>supersets</b>
      of LR(k) might be faster than those
      that take on LR(k) directly.
      In short, he assumes, as is usually the case,
      that as you gain in parsing power,
      you lose in terms of speed.
      That is a good rule of thumb,
      and a reasonable expectation,
      but it is not always true.
    </p>
    <p>A factor in this misconception may be another assumption:
      the most efficient way to parse a deterministic language
      is with a deterministic parser.
      Certainly all deterministic parsers are linear.
      And certainly all non-deterministic parsers can go super-linear,
      at which point they quickly become impractical.
      But neither of these truths directly answers the question
      of practical interest:
    </p><blockquote>
      Is there a non-deterministic parser which is linear for
      superset of the deterministic context-free grammars?
    </blockquote><p>
      This question will be answered by Joop Leo in 1991.
      The answer, surprisingly, will be
      <q>yes</q>.
    </p><h1>1968: Lewis and Stearns discover LL</h1>
    <p>When Knuth discovered the LR grammars, he announced them to
      the world with a full-blown mathematical description.
      The top-down
      grammars, which arose historically,
      have lacked such a description.
      In 1968,
      Lewis and Stearns fill that gap by defining the LL(k) grammars<footnote>
        <bibref>Lewis and Stearns 1968</bibref>.
        They are credited in Rosencrantz and Stearns (1970)
        and
        <bibref>Aho and Ullman 1972</bibref>, p. 368.
      </footnote>.
    </p>
    <h1>Terms:
      <term>LL</term>,
      <term>LR</term>,
      <term>RL</term>
      and
      <term>RR</term></h1>
    <p>When LL is added to the vocabulary of parsing, the meaning of
      <term>LR</term>
      shifts slightly. In 1965 Knuth meant LR to mean
      <q>translatable from left to right</q><footnote>
        <bibref>Knuth 1965</bibref>, p. 610.
        See on p. 611
        "corresponds with the intuitive notion of translation
        from left to right looking k characters ahead".
      </footnote>.
      But LL means
      <q>scan from the left, using left reductions</q>
      and, in response, the meaning of LR shifts to become
      <q>scan from the left, using
        right reductions</q><footnote>
        <bibref>Knuth 1971</bibref>, p. 102.
        LL and LR have mirror images: RL means
        <q>scan from the right,
          using left reductions</q>
        and RR acquires its current meaning
        of
        <q>scan from the right, using right reductions</q>.
        Practical use of these
        mirror images is rare, but it may have occurred
        in one of the algorithms in our timeline
        --
        operator expression parsing
        in the IT compiler seems to have been RL(2) with backtracking.
      </footnote>.
    </p><p>If there is a number after the parentheses in this notation for
      parsing algorithms, it usually indicates the number of tokens of
      lookahead.
      As an example, LL(1) means
      <q>scan from the
        left, using left reductions with one character of lookahead</q>.
      LL(1) will be important in what follows.
    </p>
    <h1>LL(1) and the Operator Issue</h1>
    <p>
      With
      <bibref>Knuth 1965</bibref>
      and
      <bibref>Lewis and Stearns 1968</bibref>,
      we can now restate the problem with recursive descent
      and operator expressions in precise terms:
      Recursive descent
      in its pure form,
      is LL(1).
      Arithmetic operator grammars are not LL(1) -- not even close.
      In fact neither
      <a href="#g-basic-op">BASIC-OP</a>
      or
      <a href="#g-right-op">RIGHT-OP</a>
      is LL(k) for any k.
    </p>
    <p>
      Compromises have to be made
      if we are parsing with recursive descent.
      As one result of these compromises,
      truly
      declarative versions of LL(1) are not used.
      A pure declarative
      LL(1) parser generator
      <em>could</em>
      be written, but it would not be able to parse arithmetic expressions
      properly.
    </p>
    <p>As mentioned, a common compromise is to have recursive descent parse
      arithmetic expressions as lists,
      and add associativity in post-processing.
      We are now able to look at this in more detail.
      An extended BNF grammar to recognize
      <a href="#g-basic-op">BASIC-OP</a>
      as a list is as follows:
    </p><pre id="g-elist-op"><tt>
      S  ::= E
      E  ::= T { TI }*
      TI ::= '+' T
      T  ::= F { FI } *
      FI ::= '*' F
      F  ::= number
    </tt></pre><p>
      In the above
      <tt>{ X }*</tt>
      means
      <q>zero or more occurences of X</q>.
      Expanded into pure BNF,
      and avoiding empty right hands sides,
      our operator "list" grammar becomes
      LIST-OP:
    </p><pre id="g-list-op"><tt>
      S  ::= E
      E  ::= T TL
      E  ::= T
      TL ::= TI
      TL ::= TI TL
      TI ::= '+' T
      T  ::= F FL
      T  ::= F
      FL ::= FI
      FL ::= FI FL
      FI ::= '*' F
      F  ::= number
    </tt></pre>
    <p><a href="g-list=op">LIST-OP</a>
      is LL(1),
      and therefore can be parsed directly
      by recursive descent.
    </p><h1>1968: Earley's algorithm</h1>
    <p>Jay Earley discovers the algorithm named after him<footnote>
        <bibref>Earley 1968.</bibref>
      </footnote>.
      Like the Irons
      algorithm, Earley's algorithm is Chomskyan, declarative and fully
      general. Unlike the Irons algorithm, it does not backtrack. Earley's
      algorithm is both top-down and bottom-up at once -- it uses dynamic
      programming and keeps track of the parse in tables. Earley's approach
      makes a lot of sense and looks very promising indeed, but there are
      three serious issues:</p>
    <ul>
      <li>First, there is a bug in the handling of zero-length rules.</li>
      <li>Second, it is quadratic for right recursions.</li>
      <li>Third, the bookkeeping required to set up the tables is,
        by the standards of 1968 hardware, daunting.</li>
    </ul>
    <h1>1968: Attribute grammars</h1>
    <p>Irons' synthetic attributes had always been inadequate for many
      problems.
      Until now, they had been supplemented by side effects or state
      variables.
      In 1968,
      Knuth publishes a paper on a concept he had been working for the
      previous few years: inherited attributes<footnote>
        <bibref>Knuth 1968</bibref>.
      </footnote>.
    </p>
    <h1>Term:
      <term>Inherited attributes</term></h1>
    <p>Recall that a node in parse gets its synthetic attributes from
      its children. Inherited attributes are attibutes
      that a node gets from
      its parents.
      Of course, if both inherited and synthetic attributes
      are used, there are potential circularities.
      But
      inherited attributes are powerful and, with care, the circularities
      can be avoided.</p>
    <h1>Term:
      <term>Attribute grammar</term></h1>
    <p>An attribute grammar is a grammar whose nodes may have both inherited and synthetic attributes.</p>
    <h1>1969: LALR</h1>
    <p>Since
      <bibref>Knuth 1965</bibref>
      many have taken up his challenge to find a
      practically parseable subset of the LR(k) languages.
      In 1969,
      Frank DeRemer describes a new variant of Knuth's LR
      parsing<footnote>
        <bibref>DeRemer 1969</bibref>.
        DeRemer's LALR algorithm requires only a stack and a state
        table of quite manageable size.
        LALR looks practical,
        and will go on to become the most widely used of the LR(k)
        subsets.</footnote></p>
    <h1>1969: the
      <tt>ed</tt>
      editor</h1>
    <p>Ken Thompson writes the
      <tt>ed</tt>
      editor as one of the
      first components of UNIX. At this point, regular expressions are an
      esoteric mathematical formalism. Through the
      <tt>ed</tt>
      editor
      and its descendants, regular expressions will become an everyday
      part of the working programmer's toolkit.</p>
    <h1>1972: Aho and Ullman is published</h1>
    <p>Alfred Aho and Jeffrey Ullman publish
      the first volume<footnote>
        <bibref>Aho and Ullman 1972</bibref>.
      </footnote>
      of
      their two volume textbook
      summarizing the theory of parsing.
      This book is still important. It
      is also distressingly up-to-date -- progress in parsing theory
      slowed dramatically after 1972. Aho and Ullman's version
      of Earley's algorithm includes
      a straightforward fix to the zero-length rule bug in Earley's
      original<footnote>
        <bibref>Aho and Ullman 1972</bibref>, p 321.
      </footnote>.
      Unfortunately, this fix involves adding even
      more bookkeeping to Earley's.</p>
    <p>Under the names TDPL and GTDPL, Aho and Ullman investigate
      the non-Chomksyan parsers in the Schorre lineage<footnote>
        <bibref>Aho and Ullman 1972</bibref>, pp. 456-485.
      </footnote>. They note that
      <q>it can be quite difficult to determine what language is
        defined by a TDPL parser<q><footnote>
            <bibref>Aho and Ullman 1972</bibref>, p. 466.
          </footnote>.
          That is, GTDPL parsers do whatever
          they do, and that whatever is something the programmer in general
          will not be able to describe. The best a programmer can usually
          do is to create a test suite and fiddle with the GTDPL description
          until it passes. Correctness cannot be established in any stronger
          sense.
        </q></q></p>
    <p>
      GTDPL takes the old joke that
      <q>the code is
        the documentation</q>
      one step further --
      with GTDPL nothing documents the grammar,
      not even the code.
      <comment>TODO: Move this to PEG?</comment>
    </p><p>At or around this time, rumor has it that the main line of
      development for GTDPL parsers is classified secret by the US
      government<footnote>
        http://www.wikiwand.com/en/Talk:Metacompiler/Archive_2</footnote>.
      GTDPL parsers have the property that even small changes in
      GTDPL parsers can be very labor-intensive. For some government
      contractors, GTDPL parsing provides steady work for years to
      come. Public interest in GTDPL fades.</p>
    <h1>1973: Pratt parsing</h1>
    <p>One approach to the Operator Issue has not been mentioned. We
      have noted that LL(1) cannot parse operator expressions. What about
      making the entire grammar an operator grammar? That way you don't
      have to mix algorithms.</p>
    <p>It's an idea that had already occurred to
      Samuelson and Bauer in 1959.
      They intended their operator parser as a parser for ALGOL.
    </p><p>There are many problems with switching to operator
      grammars. Operator grammar are non-Chomskyan -- the BNF no longer
      accurately describes the grammar<footnote>
        <bibref>Samuelson and Bauer 1960</bibref>
        did not even include a BNF grammar --
        just the precedence tables.
      </footnote>. Instead the BNF becomes part of
      a combined notation, and the actual grammar parsed depends also on
      precedence and semantics.
      And operator grammars have a very restricted
      form -- most practical languages are not operator grammars.</p>
    <p>But many practical grammars are almost operator grammars. And the
      Chomskyan approach has always had its dissenters.
      Vaughn Pratt was one
      of these, and discovered a new approach (the third and last in
      Theodore
      Norvell's taxonomy<footnote><bibref>Norvell 1999</bibref>)
        to operator expression parsing, one which
        some have adopted as an overall solution to their parsing problems<footnote>
          <bibref>Pratt 1973</bibref>.
        </footnote>.
      </footnote></p>
    <p>The Pratt approach, and its descendant, precedence climbing,
      is not popular as an overall strategy.
      It is most often used as an alternative to the classic
      and the shunting yard approached,
      within a recursive descent strategy.
      All
      operator expression subparsers break the Chomskyan paradigm so the
      non-Chomskyan nature of Pratt's parser is not a problem in this
      context. But with respect to the Operator Issue, like Dijkstra's
      alternative, it is an implementation choice, not a game-changer.</p>
    <h1>1975: The C compiler is converted to LALR</h1>
    <p>Bell Labs converts its C compiler from hand-written recursive
      descent to DeRemer's LALR algorithm<footnote>
        <bibref>Synder 1975</bibref>.
        See, in particular, pp. 67-68.
      </footnote>.</p>
    <h1>1977: The first dragon book is published</h1>
    <p>The first
      <q>dragon book</q><footnote>
        <bibref>Aho and Ullman 1977</bibref>.
      </footnote>
      comes out. This soon-to-become
      classic textbook is nicknamed after the drawing on the front cover,
      in which a knight takes on a dragon. Emblazoned on the knight's lance
      are the letters
      <q>LALR</q>. From here on out, to speak lightly
      of LALR will be to besmirch the escutcheon of parsing theory.</p>
    <h1>1979: Yacc is released</h1>
    <p>Bell Laboratories releases Version 7 UNIX<footnote>
        <bibref>McIlroy and Kernighan 1979</bibref>.
      </footnote>. V7 includes what is,
      by far, the most comprehensive, useable and easily available compiler
      writing toolkit yet developed.</p>
    <p>Part of the V7 toolkit is Yet Another Compiler Compiler
      (YACC)<footnote>
        S. C. Johnson, "Yet another compiler-compiler", in Volume 2A
        of
        <bibref>McIlroy and Kernighan 1979</bibref>.
      </footnote>. YACC is LALR-powered. Despite its name, YACC is the first
      compiler-compiler in the modern sense. For some useful languages, the
      process of going from Chomskyan specification to executable is fully
      automated. Most practical languages, including the C language and
      YACC's own input language, still require manual hackery. Nonetheless,
      after two decades of research, it seems that the parsing problem
      is solved.</p>
    <h1>1987: Perl 1 is released</h1>
    <p>Larry Wall introduces Perl 1<footnote>
        Larry Wall, 'v13i001: Perl, a "replacement" for awk and sed, Part01/10'. Newsgroup: comp.sources.unix.
        1988-02-01).
        [https://groups.google.com/forum/#!topic/comp.sources.unix/Njx6b6TiZos].
      </footnote>. Perl embraces complexity like no
      previous language. Larry uses YACC and LALR very aggressively --
      to my knowledge more aggressively than anyone before or since.</p>
    <h1>1990: Monads</h1>
    <p>
      Wadler starts to introduce monads to the functional programming community.
      As one example he converts his previous efforts at function parsing
      techniques<footnote>
        Philip Wadler,
        "Comprehending Monads"
        In
        <cite>Proceedings of the 1990 ACM conference on LISP and functional programming</cite>
        (LFP '90).
        ACM, New York, NY, USA, 61-78. DOI=http://dx.doi.org/10.1145/91556.91592
      </footnote>.
      The parsing technique is pure recursive descent.
      The examples shown for monadic parsing are very simple,
      and do not include operator expressions.
      In his earlier non-monadic work, Wadler uses a very restricted form of operator expression:
      His grammar avoided left recursion by using parentheses,
      and operator precedence was not implemented<footnote>
        Wadler 1985.</footnote>.
    </p>
    <h1>1991: Leo's speed-up of Earley's algorithm</h1>
    <p>Joop Leo discovers a way of speeding up right recursions in
      Earley's algorithm<footnote>
        <bibref>Leo 1991</bibref>.
      </footnote>. Leo's algorithm is linear for just about every
      unambiguous grammar of practical interest, and many ambiguous ones
      as well. In 1991 hardware is six orders of magnitude faster than
      1968 hardware, so that the issue of bookkeeping overhead had receded
      in importance. This is a major discovery. When it comes to speed,
      the game has changed in favor of the Earley algorithm.</p>
    <p>But Earley parsing is almost forgotten. Twenty years will
      pass before anyone writes a practical implementation of Leo's
      algorithm.</p>
    <h1>The 1990's</h1>
    <p>Earley's is forgotten. So everyone in LALR-land is content,
      right? Wrong. Far from it, in fact. Users of LALR are making
      unpleasant discoveries. While LALR automatically generates their
      parsers, debugging them is so hard they could just as easily write the
      parser by hand. Once debugged, their LALR parsers are fast for correct
      inputs. But almost all they tell the users about incorrect inputs
      is that they are incorrect. In Larry's words, LALR is
      <q>fast
        but stupid</q><footnote>
        Perl 6 IRC, August 30, 2014.
        [https://irclog.perlgeek.de/perl6/2014-08-30#i_9271280]
      </footnote>.
    </p>
    <h1>1992: Combinator parsing</h1>
    <p>Combinators are parsers which can be
      <q>combined</q>
      to compose other parsers.
      This allows you to have an algebra of parsers.
      It's an extremely attractive idea,
      and suggestive of great power,
      but in fact underneath it is nothing but recursive descent.
    </p>
    <p>
      Combinator parsing
      was introduced in two 1992 papers<footnote>
        Some source consider
        <bibref>Wadler 1985</bibref>
        an
        early presentation of the ideas of combinator parsing.
        In assigning priority here,
        I follow Grune and Jacobs 2008 (p. 564).
      </footnote>.
      Of more interest
      to us is the one by Hutton, which focuses on combinator parsing<footnote>
        The paper which is devoted to parsing is
        <bibref>Hutton 1992</bibref>.
        The other paper, which centers on combinators as a programming
        paradigm, is
        Frost, Richard A. Constructing programs as executable attribute
        grammars. Computer J., 35(4):376–389, 1992.
        Frost only mentions parsing in one paragraph, and
        that focuses on implementation issues.
        Some of his grammars include operator expressions,
        but these avoid left recursion,
        and implement precedence and associativity,
        using parentheses.
      </footnote>.
      Combinators will become important in programming languages and
      semantics. But in terms of parsing algorithms, Hutton introduces
      nothing new -- wrapped up in an attribute grammar; underlying the
      exciting new mathematics is the decades-old recursive descent,
      unchanged.
    </p><p>Hutton's example language is very basic --
      his expressions require left association,
      but not precedence.
      Left association is implemented by post-processing.
    </p>
    <p>
      Hutton's main presentation does not use monads.
      In his final pages, however,
      Hutton points out
      <q>combinator parsers give rise to a monad</q>
      and shows
      how his presentation could be rewritten in a form
      closely related to monads<footnote>
        <bibref>Hutton 1992</bibref>, pp. 19-20.
      </footnote>.
    </p>
    <h1>1995: Wadler's monads</h1><p>
      Today monads are a hot topic.
      In their adaptation into the world of
      programming,
      the 1995 article by Wadler
      <footnote>
        Wadler Philip. (1995) Monads for functional programming. In:
        Jeuring J., Meijer E. (eds) Advanced Functional Programming. AFP
        1995. Lecture Notes in Computer Science, vol 925. Springer,
        Berlin, Heidelberg.
      </footnote>
      is a turning point.
      One of Wadler's examples is a parser based on
      monads and combinator parsing.
      From here on out,
      an combinator parsing will become the standard
      example of monad programming.
    </p>
    <p>In its monadic form,
      the already attractive technique of combinator parsing is extremely elegant
      and certainly seems as if it *should* solve all parsing problems.
      But, once again, it is still recursive descent.
      Wadler handles left recursion by rewriting into
      right recursive list, and re-processing the list
      into left recursive form.
      Wadler's example grammar only has one operator,
      so the issue of precedence does not arise.
    </p>
    <p>
      Instead of one paradigm to solve them all,
      Wadler ends up with a two layered approach,
      with a hackish bottom layer and an awkward interface.
      This undercuts
      the simplicity and elegance of the combinator approach.
    </p>
    <h1>1996: Hutton and Meijer on cominator parsing</h1><p>
      <comment>TODO: Expand with stuff on operator issue</comment>
      Wadler had used parsing as an example to motivate
      his presentation of monads.
      In 1996, Graham Hutton and Erik Meijer<footnote>
        <bibref>Hutton and Meijer 1996</bibref>
      </footnote>
      take the opposite perspective -- they
      write a paper on combinator parsing that could
      <q>also be viewed as a first introduction to the use
        of monads in programming.</q><footnote>
        <bibref>Hutton and Meijer 1996</bibref>,
        p. 3.
      </footnote>
    </p>
    <h1>1999: Haskell
      <cite>Gentle Introduction</cite></h1>
    <p>
      [ TODO: Finish. ]
      <footnote>
        Hudak, Peterson and Fasel,
        "A Gentle Introduction to Haskell 98",
        1999.
      </footnote>
    </p><h1>The Operator Issue as of 1999</h1>
    <p>
      As we have seen,
      the contributions of combinators parsing,
      and monads to parsing theory itself are minimal.
      But effect on
      interfaces and the way parsing is seen is large
      and likely to grow.
      And following this literature has allowed us to
      see what is consider the state of the parsing art
      at the time.
    </p>
    <h1>2000: The Perl 6 effort begins</h1>
    <p>Larry Wall decides on a radical reimplementation of Perl --
      Perl 6. Larry does not even consider using LALR again.</p>
    <h1>2002: Aycock and Horspool solve Earley's zero-length rule problem</h1>
    <p>John Aycock and R. Nigel Horspool publish their attempt at a
      fast, practical Earley's parser<footnote>
        <bibref>Aycock and Horspool 2002</bibref>.
      </footnote>. Missing from it is Joop Leo's
      improvement -- they seem not to be aware of it. Their own speedup
      is limited in what it achieves and the complications it introduces
      can be counter-productive at evaluation time. But buried in their
      paper is a solution to the zero-length rule bug. And this time the
      solution requires no additional bookkeeping.</p>
    <h1>2004: PEG</h1>
    <p>Ford<footnote>
        <bibref>Ford 2004</bibref>.
        See also
        <bibref>Ford 2002</bibref>.
      </footnote>fills this gap by
      repackaging the nearly-forgotten GTDPL. Ford adds packratting,
      so that PEG is always linear, and provides PEG with an attractive
      new syntax.</p>
    <p>Implementers by now are avoiding YACC, but the demand for
      declarative parsers remains. PEG is not, in fact, declarative,
      but it uses the same BNF notation, and many users don't know the
      difference. And nothing has been done to change
      <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2015/03/peg.html">the problematic behaviors</a>
      of GTDPL.</p>
    <comment>TODO: Remark on the use of BNF in standards here?</comment>
    <h1>2006: GNU C reverts to recursive descent</h1>
    <p>
      <q>The old Bison-based C and Objective-C parser has been replaced by a new, faster hand-written recursive-descent
        parser.<q><footnote>
            "GCC 4.1 Release Series: Changes, New Features, and Fixes"
            [http://gcc.gnu.org/gcc-4.1/changes.html]
          </footnote>
          Bison, an LALR parser, is the direct descendant of Steve Johnson's YACC.
        </q></q></p>
    <p>
      With this single line in middle of a change list hundreds of lines long.
      GNU announces a major event in parsing history.
      For three decades, the industry's flagship C compilers have
      used LALR as their parser -- proof of the claim that LALR and serious
      parsing are equivalent.
      Now, GNU replaces LALR with the technology
      that it replaced a quarter century earlier: recursive descent.</p>
    <h1>2010: Leo 1991 is implemented</h1><p>
    </p><p>Jeffrey Kegler (the author of this timeline) releases the first
      practical implementation of Joop Leo's algorithm<footnote>
        [http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2010/06/marpa-is-now-on-for-right-recursions.html].
      </footnote>.
      The new parser, named Marpa, is general, declarative
      and linear for all the grammars discussed in
      <bibref>Knuth 1965</bibref>, that is, it is
      linear for the LR(k) grammars for all finite k<footnote>
        In fact,
        <bibref>Leo 1991</bibref>
        proves his algorithm is linear for the LR-regular
        grammars -- the LR grammars with infinite lookahead, so long as the lookahead
        is a regular expresion.
        It is not known (in fact not decidable, see
        <bibref>Leo 1991</bibref>, p. 175.),
        just how large a class of grammars
        Marpa is linear for.
        But it is known that there are both ambiguous and unambiguous grammars
        for which Marpa is not linear.
        In the general case, Marpa obeys the bounds of Earley's algorithm --
        it is worst-case O(n**2) for unambiguous grammars;
        and worst-case O(n**3) for ambiguous grammars.
        On the other hand, Marpa follows
        <bibref>Leo 1991</bibref>
        in being linear for many ambiguous grammars,
        including those of most practical interest.
      </footnote>
      This means it is also linear for most of the grammar classes we have discussed in this
      timeline.
    </p><ul>
      <li>LR(1) grammars.</li>
      <li>LL(k) grammars for all finite k, including LL(1) grammars.</li>
      <li>Operator expression grammars<footnote>
          In the literature operator expression grammars are more often
          call simply
          <q>operator grammars</q>.
        </footnote>.
        <bibref>Leo 1991</bibref>
        does not address the zero-length
        rule problem.
        Kegler solves it by adopting the solution of
        <bibref>Aycock and Horspool 2002</bibref>.
      </li>
    </ul>
    <p>This is a vast class of grammars,
      and it has the important feature that a programmer
      (and, for the benefit of automatic grammar generation,
      a program)
      can readily determine if their grammar is linear under Marpa.
      Marpa will parse a grammar in linear time, if
    </p><ul id="loc-linearity-rules">
      <li>It is unambiguous.<footnote>
          In the general case, ambiguity is undecidable, but
          in practice it is usually straight-forward for a programmer
          to determine that the grammar he wants is unambiguous.
          Note the while the general case is undecidable,
          Marpa will tell the programmer is a parse (a grammar with a
          particular input) is ambiguous and, since it parses ambiguous
          grammars, produces an error message showing where the ambiguity is.
        </footnote>
      </li>
      <li>It has no unmarked middle recursions<footnote>A
          <q>marker</q>, in this sense, is something in the input
          that shows where the middle of a middle recursion is,
          without having to go to the end and count back.
          Whether or not a grammar is unmarked is, in general,
          an undecidable problem.
          But in practice, there's a easy rule:
          if a person can
          <q>eyeball</q>
          the moddie of a long
          middle recursion, and does not need to count from both
          ends to find it,
          the grammar is
          <q>marked</q>.
        </footnote>.
      </li>
      <li>It has no ambiguous right recursions<footnote>
          This is a very pedantic requirement,
          which practical programmers can in fact ignore.
          It is an open question whether there
          <em>are</em>
          any unambiguous
          grammars with ambiguous right recursions.
          In any case, in practice, a programmer,
          in determining that his grammar is ambiguity-free
          will spot any unambiguous right recursions as well.
        </footnote>
      </li>
    </ul>
    <p>
      Finally in an effort to combine the best of declarative and hand-written parsing,
      Kegler reorders Earley's parse engine so that it
      allows procedural logic.
      As a side effect,
      this gives Marpa excellent error-handling properties.
    </p>
    <comment>TODO: Add more</comment>
    <h1>2012: General precedence parsing</h1>
    <p>
      While the purely
      precedence-driven parsing of
      <bibref>Samuelson and Bauer 1960</bibref>
      and
      <bibref>Pratt 1973</bibref>
      never caught on,
      the use of precedence in parsing remains popular.
      In fact, precedence tables often occur in documentation
      and standards,
      which is a clear sign that they too can contribute
      to human-readable but precise descriptions of grammars.
    </p>
    <p>
      <comment>TODO: Finish</comment>
      In 2012, Kegler release a version of Marpa which allows
      the user to mix Chomskyan and precedence parsing at will<footnote>
        [http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2012/08/precedence-parsing-made-simpler.html].
      </footnote>.
      Marpa's
      <q>precedenced statements</q>
      describe expressions
      in terms of precedence and association, but they strictly
      translate into Chomskyan form,
      so a user writing a precedenced statement
      does not lose the benefits
      of Chomskyan parsing.
      If
      <q>precedenced statements</q>
      are added to a grammar that Marpa parses in linear
      time, then the grammar remains linear as long as the precedenced statements
      are (after precedence is taken in consideration) unambiguous<footnote>
        Internally, Marpa rewrites precedenced statements into BNF.
        Marpa's rewrite does not use middle recursions
        and does not introduce any new ambiguities
        so that, following its
        <a href="#loc-linearity-rules">linearity
          rules</a>, the result must be linear.
      </footnote>.
    </p><p>Here is an example of a Marpa
      <q>precedenced statement</q>:
      <footnote>
        In this, precedence goes from tightest to loosest.
        Single bars (<tt>|</tt>) separate RHS's of equal precedence.
        Double bars (<tt>||</tt>) separate RHS's of different precedence --
        precedence goes from tightest to loosest.
        Associativity default to left,
        and exceptions are indicated by the value of the
        <tt>assoc</tt>
        adverb.
        Note that these
        <q>precedenced statements</q>,
        like all of Marpa's DSL,
        are parsed by Marpa itself.
        The form shown has the semantic adverbs removed for clarity.
        The original is part of the
        <a href="https://metacpan.org/pod/Marpa::R2">
          Marpa::R2</a>
        test suite
        and can be found in
        <a href=
	    "https://metacpan.org/pod/release/JKEGL/Marpa-R2-4.000000/pod/Scanless/DSL.pod"
	    >
          the Marpa documentation</a>.
      </footnote>
    </p>
    <pre><tt>
              Expression ::=
                 Number
              |  '(' Expression ')' assoc => group
              || Expression '**' Expression assoc => right
              || Expression '*' Expression
              |  Expression '/' Expression
              || Expression '+' Expression
              |  Expression '-' Expression
            </tt></pre>
    <h1>Operator Expressions as of 2012</h1>
    <comment>TODO: Do this.</comment>
    <h1>Sources</h1>
    <p>This bibliography is incomplete --
      full details of some documents
      are given in the individual footnotes.
    </p>
    <p>
      <bibid>Aho and Ullman 1972</bibid>:
      Alfred V. Aho and Jeffrey D. Ullman,
      <cite>The Theory of Parsing, Translation and Compiling:
        Volume I: Parsing</cite>, Prentice Hall, Englewood Cliffs, N.J., 1972.
    </p>
    <p>
      <bibid>Aho and Ullman 1973</bibid>:
      Alfred V. Aho and Jeffrey D. Ullman,
      <cite>The Theory of Parsing, Translation and Compiling:
        Volume II: Compiling</cite>, Prentice Hall, Englewood Cliffs, N.J., 1973.
    </p>
    <p>
      <bibid>ALGOL 60</bibid>:
      J.W. Backus, F.L. Bauer, J.Green, C. Katz, J. McCarthy
      P. Naur, A.J. Perlis, H. Rutishauser, K. Samuelson, B. Vauquois
      J.H. Wegstein, A. van Wijngaarden and M. Woodger,
      edited by Peter Naur,
      <cite>Revised Report on the Algorithmic Language Algol 60</cite>.
      [http://homepages.cs.ncl.ac.uk/cliff.jones/publications/OCRd/BBG63.pdf]
    </p>
    <p>
      <bibid>Earley 1968</bibid>:
      Jay Earley,
      <cite>An Efficient Context-free Parsing Algorithm</cite>,
      PhD thesis, Carnegie-Mellon Univ., Pittsburg, Pa., 1968.
    </p>
    <p>
      <bibid>Aho and Ullman 1977</bibid>:
      Alfred V. Aho and Jeffrey D. Ullman,
      <cite>Principles of compiler design</cite>,
      Addision-Wesley,
      Reading, Mass. (1977)
      The dragon on the cover is green,
      and this classic first edition
      is sometimes called the
      <q>green dragon book" to distinguish
        it from its second edition,
        which had a red dragon on the cover.
      </q></p>
    <p>
      <bibid>Aycock and Horspool 2002</bibid>:
      John Aycock and R. Nigel Horspool, "Practical Earley parsing",
      <cite>Computer J.</cite>
      45(6):620–630. (2002)
    </p>
    <p>
      <bibid>Backus 1959</bibid>:
      John Backus,
      "The Syntax and Semantics of the Proposed International Algebraic
      Language of the Zurich ACM-GAMM Conference. Proceedings of the
      International Conference on Information Processing",
      UNESCO, pp.125-132.
      [http://www.softwarepreservation.org/projects/ALGOL/paper/Backus-Syntax_and_Semantics_of_Proposed_IAL.pdf]
      [http://www.softwarepreservation.org/projects/ALGOL/paper/Backus-ICIP-1959.pdf]
      (1959)
    </p>
    <p>
      <bibid>Backus 1980</bibid>:
      John Backus,
      <cite>Programming in America in the 1950s- Some Personal Impressions</cite>
      [http://www.softwarepreservation.org/projects/FORTRAN/paper/Backus-ProgrammingInAmerica-1976.pdf]
      10.1016/B978-0-12-491650-0.50017-4.
      (1980)
    </p>
    <p>
      <bibid>Chomsky 1956<bibid>
          Noam Chomsky,
          "Three models for the description of language",
          <cite>IEEE Trans. Inform.  Theory</cite>,
          2(3):113–124.
          (1956)
        </bibid></bibid></p>
    <p>
      <bibid>Chomsky 1957</bibid>:
      Noam Chomsky
      <cite>Syntactic Structures</cite>,
      The Hague/Paris: Mouton, ISBN 978-3-11-021832-9.
      (1957)
    </p>
    <p>
      <bibref>DeRemer 1969</bibref>:
      Franklin Lewis DeRemer,
      "Practical Translators for LR(k) Languages"
      PhD thesis, MIT, Cambridge, Mass., Sept. 1969.
    </p>
    <p>
      <bibid>Ford 2002</bibid>:
      Bryan Ford, "Packrat parsing: a practical linear-time algorithm with backtracking", Master’s thesis,
      Massachusetts Institute of Technology. (September 2002.)
      [http://pdos.csail.mit.edu/papers/packrat-parsing:ford-ms.pdf.]
    </p>
    <p>
      <bibid>Ford 2004</bibid>:
      Bryan Ford,
      "Parsing expression grammars: A recognition-based syntactic foundation",
      in
      <cite>Proceedings
        of the 31st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages,
        POPL 2004 (Venice, Italy, 14–16 January 2004)</cite>, N. D. Jones and X. Leroy, Eds., ACM, pp. 111–122.
      (January 2004.)
    </p>
    <p>
      <bibid>Glennie 1960</bibid>:
      A. E. Glennie,
      "On the Syntax Machine and the Construction of a Universal Compiler"
      Computation Center, Carnegie Institute of Technology,
      <cite>Technical Report No. 2</cite>
      (10 July 1960).
      [http://www.chilton-computing.org.uk/acl/literature/reports/p024.htm]
    </p>
    <p>
      <bibid>Grune and Jacobs 2008</bibid>:
      Grune, D., & Jacobs, C. J. H. (2008).
      <cite>Parsing Techniques: A Practical
        Guide, 2nd edition</cite>.
      (<cite>Monographs in Computer Science</cite>). New York, USA:
      Springer.
      When it comes to determining who first discovered an idea,
      the literature often disagrees.
      In such cases, I have often deferred
      to the judgement of Grune and Jacobs.
    </p>
    <p>
      <bibid>Hutton 1992</bibid>:
      Hutton, "Higher order functions for parsing",
      Journal of Functional Programming 2(3):323-343, July 1992.
    </p>
    <p>
      <bibid>Hutton and Meijer 1996</bibid>:
      Graham Hutton and Erik Meijer.
      <cite>Technical Report NOTTCS-TR-96-4</cite>,
      Department of Computer Science, University of Nottingham, 1996.
    </p>
    <p><bibid>Irons 1961</bibid>:
      Irons, E. T. A syntax-directed compiler for ALGOL 60. Commun. ACM,
      4(1):51–55, Jan. 1961.
    </p>
    <p>
      <bibid>Kleene 1951</bibid>
      Stephen C.  Kleene,
      "Representation of Events in Nerve Nets and Finite Automata",
      Project Rand Resarch Memorandum RM-704.
      (15 December 1951)
      [https://www.rand.org/content/dam/rand/pubs/research_memoranda/2008/RM704.pdf]
    </p>
    <p>
      <bibid>Knuth 1965</bibid>:
      D. E. Knuth, "On the translation of languages from left to right"
      <cite>Information and Control</cite>, Vol. 8, Issue: 6, Page: 607-639 (1965).
    </p>
    <p>
      <bibid>Knuth 1968</bibid>:
      Donald E. Knuth, "Semantics of context-free languages"
      Math. Syst. Theory, 2(2):127– 145, (1968).
    </p>
    <p>
      <bibid>Knuth 1971</bibid>:
      Donald E.  Knuth, "Top-down syntax analysis",
      <cite>Acta Inform.</cite>,
      1:79–110.
      (1971)
    </p>
    <p>
      <bibid>Knuth and Pardo 1976</bibid>:
      D. E. Knuth and L. Trabb Pardo, "The Early Development of
      Programming Languages,"
      <cite>Report STAN-CS-76-562</cite>, Computer Science Dept.,
      Stanford Univ., Stanford, Calif. (Aug. 1976).
      [http://bitsavers.org/pdf/stanford/cs_techReports/STAN-CS-76-562_EarlyDevelPgmgLang_Aug76.pdf ].
    </p>
    <p>
      <bibid>Leo 1991</bibid>:
      Joop M. I. M. Leo,
      "A general context-free parsing algorithm running in linear time on
      every LR(k) grammar without using lookahead",
      Theoret. Comput. Sci., 82:165–176, 1991.
    </p>
    <p>
      <bibid>Lewis and Stearns 1968</bibid>:
      P. M. Lewis, II and R. E .Stearns, "Syntax-directed transduction",
      <cite>Journal of the ACM</cite>,
      15(3):465– 488 (1968).
    </p>
    <p>
      <bibid>Lucas 1961</bibid>:
      Lucas, P. Die Strukturanalyse von Formelnübersetzern / analysis of
      the structure of formula translators. Elektronische Rechenanlagen,
      3(11.4):159–167, 1961, (in German).
      (September 1961)
    </p>
    <p>
      <bibid>Mascrenhas et al 2014</bibid>
      Fabio Mascarenhas, S&egrave;rgio Medeiros and
      Roberto Ierusalimschy,
      "On the Relation between Context-Free Grammars
      and Parsing Expression Grammars",
      <cite>Science of Computer Programming</cite>
      89, pp. 235 - 250.
      [https://arxiv.org/abs/1304.3177]
      (2014)
    </p>
    <p>
      <bibid>Norvell 1999</bibid>:
      Theodore Norvell,
      "Parsing Expressions by Recursive Descent",
      https://www.engr.mun.ca/~theo/Misc/exp_parsing.htm.
      (1999)
    </p>
    <p>
      <bibid>Pratt 1973</bibid>:
      Vaughan R. Pratt, "Top down operator precedence", in
      <cite>First
        ACM Symposium on Principles of Programming Languages</cite>,
      pages 41–51, ACM. (Oct. 1973.)
    </p>
    <p>
      <bibid>Samuelson and Bauer 1959</bibid>:
      K. Samelson and F. L. Bauer,
      "Sequentielle Formel&uuml;bersetzung",
      <cite>Elektronishce Rechenanlagen</cite>,
      1, 176-182. (1959)
    </p>
    <p>
      <bibid>Samuelson and Bauer 1960</bibid>:
      K. Samelson and F. L. Bauer, "Sequential formula translation",
      <cite>Communications of the ACM</cite>,
      3(2):76–83, Feb. 1960.
      A translation of
      <bibref>Samuelson and Bauer 1959</bibref>.
    </p>
    <p>
      <bibid>Schorre 1964</bibid>:
      "Meta II: A Syntax-oriented compiler writing language",
      http://ibm-1401.info/Meta-II-schorre.pdf
    </p>
    <p>
      <bibid>Shannon 1948</bibid>:
      Claude E Shannon, Claude E.
      "A Mathematical Theory of Communication",
      <cite>Bell System Technical Journal</cite>,
      27 (3): 379–423
      and
      27 (4): 623–666. (July and October 1948)
    </p>
    <p>
      <bibid>Snyder 1975</bibid>:
      Alan Snyder, "A Portable Compiler for the Language C",
      MIT, Project MAC, Cambridge MA. (May 1975.)
    </p>
    <p>
      <bibid>McIlroy and Kernighan 1979</bibid>:
      M. D. McIlroy and B. W. Kernighan, editors,
      <cite>UNIX Programmer's Manual</cite>, Seventh Edition, Volume 1,
      Bell Labs, Murray Hill, New Jersey.
      (January 1979.)
    </p>
    <p><bibid>Wadler 1985</bibid>:
      Philip Wadler, "How to Replace Failure by a List of Successes: A method
      for exception handling, backtracking, and pattern matching in lazy
      functional languages". in J-P Jouannaud (ed.),
      <cite>Functional Programming
        Languages and Computer Architecture</cite>.
      <cite>Lecture Notes in Computer Science</cite>, 1985
      vol. 201, Springer-Verlag GmbH, pp. 113-128. DOI: 10.1007/3-540-15975-4_33.
    </p>
  </body>
</html>
